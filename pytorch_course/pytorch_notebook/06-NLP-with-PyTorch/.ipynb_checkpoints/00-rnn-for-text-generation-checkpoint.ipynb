{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3749fc82-c5c6-4e2f-88d9-e7114698ac0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9089a985-5946-4d93-8597-38a47250ce25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85ff2f6a-e065-4a7f-88dc-0a2db7e9b43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Data/shakespeare.txt','r',encoding='utf8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7b15621-fdab-443f-baa5-165b67bd0042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c32051b-c05a-4767-b13e-ec772493d9b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                     1\n",
      "  From fairest creatures we desire increase,\n",
      "  That thereby beauty's rose might never die,\n",
      "  But as the riper should by time decease,\n",
      "  His tender heir might bear his memory:\n",
      "  But thou contracted to thine own bright eyes,\n",
      "  Feed'st thy light's flame with self-substantial fuel,\n",
      "  Making a famine where abundance lies,\n",
      "  Thy self thy foe, to thy sweet self too cruel:\n",
      "  Thou that art now the world's fresh ornament,\n",
      "  And only herald to the gaudy spring,\n",
      "  Within thine own bud buriest thy content,\n",
      "  And tender churl mak'st waste in niggarding:\n",
      "    Pity the world, or else this glutton be,\n",
      "    To eat the world's due, by the grave and thee.\n",
      "\n",
      "\n",
      "                     2\n",
      "  When forty winters shall besiege thy brow,\n",
      "  And dig deep trenches in thy beauty's field,\n",
      "  Thy youth's proud livery so gazed on now,\n",
      "  Will be a tattered weed of small worth held:  \n",
      "  Then being asked, where all thy beauty lies,\n",
      "  Where all the treasure of thy lusty days;\n",
      "  To say within thine own deep su\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "743add13-1a8c-41d4-b5e8-a88bc9d9fd5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5445609"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e426e55-2346-4ded-a5ca-ade50af7153a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_characters = set(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7b1d7e1-a857-46a6-a3c4-4c8664f46d2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b53b4fa7-405d-426f-9b37-57e766a4b81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of value ---> letter\n",
    "decoder = dict(enumerate(all_characters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3927b319-796d-4e36-8b8d-090b81ca6b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "794f5a8b-6f1b-485f-8a9c-4c83cad02cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# letter ---> corresponding number\n",
    "encoder = {char: idx for idx,char in decoder.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a24b2c30-1a29-4ff5-8ab2-8c4c6ff1372f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "887e0feb-52a0-4420-a6dc-21500741ceb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text = np.array([encoder[char] for char in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "955792cc-ed99-4c1c-8aab-edcb1ca65be5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([41, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26,\n",
       "       26, 26, 26, 26, 26,  2, 41, 26, 26,  5, 74, 83, 47, 26, 28, 67, 38,\n",
       "       74,  7, 78,  3, 26, 14, 74,  7, 67,  3, 75, 74,  7, 78, 26, 39,  7,\n",
       "       26, 73,  7, 78, 38, 74,  7, 26, 38, 48, 14, 74,  7, 67, 78,  7, 30,\n",
       "       41, 26, 26, 52, 17, 67,  3, 26,  3, 17,  7, 74,  7, 53, 11, 26, 53,\n",
       "        7, 67, 75,  3, 11, 55, 78, 26, 74, 83, 78,  7, 26, 47, 38, 51, 17,\n",
       "        3, 26, 48,  7, 27,  7, 74, 26, 73, 38,  7, 30, 41, 26, 26, 80, 75,\n",
       "        3, 26, 67, 78, 26,  3, 17,  7, 26, 74, 38, 42,  7, 74, 26, 78, 17,\n",
       "       83, 75, 77, 73, 26, 53, 11, 26,  3, 38, 47,  7, 26, 73,  7, 14,  7,\n",
       "       67, 78,  7, 30, 41, 26, 26,  6, 38, 78, 26,  3,  7, 48, 73,  7, 74,\n",
       "       26, 17,  7, 38, 74, 26, 47, 38, 51, 17,  3, 26, 53,  7, 67, 74, 26,\n",
       "       17, 38, 78, 26, 47,  7, 47, 83, 74, 11, 33, 41, 26, 26, 80, 75,  3,\n",
       "       26,  3, 17, 83, 75, 26, 14, 83, 48,  3, 74, 67, 14,  3,  7, 73, 26,\n",
       "        3, 83, 26,  3, 17, 38, 48,  7, 26, 83, 39, 48, 26, 53, 74, 38, 51,\n",
       "       17,  3, 26,  7, 11,  7, 78, 30, 41, 26, 26,  5,  7,  7, 73, 55, 78,\n",
       "        3, 26,  3, 17, 11, 26, 77, 38, 51, 17,  3, 55, 78, 26, 28, 77, 67,\n",
       "       47,  7, 26, 39, 38,  3, 17, 26, 78,  7, 77, 28, 61, 78, 75, 53, 78,\n",
       "        3, 67, 48,  3, 38, 67, 77, 26, 28, 75,  7, 77, 30, 41, 26, 26,  0,\n",
       "       67, 70, 38, 48, 51, 26, 67, 26, 28, 67, 47, 38, 48,  7, 26, 39, 17,\n",
       "        7, 74,  7, 26, 67, 53, 75, 48, 73, 67, 48, 14,  7, 26, 77, 38,  7,\n",
       "       78, 30, 41, 26, 26, 52, 17, 11, 26, 78,  7, 77, 28, 26,  3, 17, 11,\n",
       "       26, 28, 83,  7, 30, 26,  3, 83, 26,  3, 17, 11, 26, 78, 39,  7,  7,\n",
       "        3, 26, 78,  7, 77, 28, 26,  3, 83, 83, 26, 14, 74, 75,  7, 77, 33,\n",
       "       41, 26, 26, 52, 17, 83, 75, 26,  3, 17, 67,  3, 26, 67, 74,  3, 26,\n",
       "       48, 83, 39, 26,  3, 17,  7, 26, 39, 83, 74, 77, 73, 55, 78, 26, 28,\n",
       "       74,  7, 78, 17, 26, 83, 74, 48, 67, 47,  7, 48,  3, 30, 41, 26, 26,\n",
       "       81, 48, 73, 26, 83, 48, 77, 11, 26, 17,  7, 74, 67, 77, 73, 26,  3,\n",
       "       83, 26,  3, 17,  7, 26, 51, 67, 75, 73, 11, 26, 78, 42, 74, 38, 48,\n",
       "       51, 30, 41, 26, 26, 62, 38,  3, 17, 38, 48, 26,  3, 17, 38, 48,  7,\n",
       "       26, 83, 39, 48, 26, 53, 75])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6fc5fcc-c7b2-4477-92d3-3e9f8b3b9435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'q'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder[43]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2dd85ce2-036e-4db3-b5a6-85fff031040b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(encoded_text, num_uni_chars):\n",
    "    # encoded text ---> batch of encoded text\n",
    "    # num_uni_chars ---> len(set(text))\n",
    "\n",
    "    one_hot = np.zeros((encoded_text.size, num_uni_chars))\n",
    "\n",
    "    # convert data type\n",
    "    one_hot = one_hot.astype(np.float32)\n",
    "\n",
    "    # set values to 1 for each corresponding character (i.e. 0, 0, 0, 0, ..., 1(43rd position), 0, 0, ...) for ' ' (space)\n",
    "    one_hot[np.arange(one_hot.shape[0]), encoded_text.flatten()] = 1.0\n",
    "\n",
    "    one_hot = one_hot.reshape((*encoded_text.shape, num_uni_chars))\n",
    "\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df9fccfd-3561-4784-bb3a-69de41e8f557",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array([1,2,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "590c7885-a7ea-475d-89ea-62dd882d751a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d485e21-9816-489e-bf12-43e3f73de37c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for idx 1 of row 1 -> 1, idx 2 of row 2 -> 1, and idx 0 of row 2 -> 1\n",
    "one_hot_encoder(arr, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2ffe010-2a5c-4b5d-8f50-b692ac7c8b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = np.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27dfbd8f-6706-4ded-a92d-2fe0fbca32fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "856b6753-5d8a-45d7-b665-6e871cf361dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [2, 3],\n",
       "       [4, 5],\n",
       "       [6, 7],\n",
       "       [8, 9]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_text.reshape((5,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "597e6839-b7fc-4856-827a-0a2a25823191",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(encoded_text, samp_per_batch=10, seq_len=50):\n",
    "    # X: encoded text of length seq_len\n",
    "    # ex: [0, 1, 2]\n",
    "    #     [1, 2, 3]\n",
    "    \n",
    "    # Y: encoded text shifted by one\n",
    "    # ex: [1, 2, 3]\n",
    "    #     [2, 3, 4]\n",
    "    \n",
    "    char_per_batch = samp_per_batch * seq_len # total number of chars per batch\n",
    "    num_batches_avail = int(len(encoded_text)/char_per_batch) # how many batches can we make\n",
    "\n",
    "    # cut off the end of the encoded text, that won't fit evenly into a batch (remove last few chars, up to 49)\n",
    "    encoded_text = encoded_text[:num_batches_avail*char_per_batch]\n",
    "\n",
    "    encoded_text = encoded_text.reshape((samp_per_batch,-1))\n",
    "\n",
    "    for n in range(0, encoded_text.shape[1], seq_len):\n",
    "        x = encoded_text[:,n:n+seq_len]\n",
    "\n",
    "        # zero array to the same shape as x\n",
    "        y = np.zeros_like(x)\n",
    "\n",
    "        try:\n",
    "            y[:,:-1] = x[:,1:]\n",
    "            y[:,-1] = encoded_text[:,n+seq_len]\n",
    "        except:\n",
    "            y[:,:-1] = x[:,1:]\n",
    "            y[:,-1] = encoded_text[:,0]\n",
    "\n",
    "        yield x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "85f478cb-f0c6-4cd1-ba13-a110ea4827d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = np.arange(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e59edef4-2ee4-4e65-9186-4d2d4085f1bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0cbbdf8b-aac6-4d46-844a-f75fd93db86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_generator = generate_batches(sample_text, samp_per_batch=2, seq_len=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1e09e595-a6af-47e9-9a70-60c8df8a662b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(batch_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "70e63fbb-49ce-44de-acb5-22d9ed62b802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2,  3,  4],\n",
       "       [10, 11, 12, 13, 14]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e0969d66-6194-4402-bb94-7617ecdc2078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3,  4,  5],\n",
       "       [11, 12, 13, 14, 15]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b0d3b6e0-2378-4784-a184-9b9ac046dc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharModel(nn.Module):\n",
    "    def __init__(self, all_chars, num_hidden=256, num_layers=4, drop_prob=0.5, use_gpu=False):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.num_layers = num_layers\n",
    "        self.num_hidden = num_hidden\n",
    "        self.use_gpu = use_gpu\n",
    "\n",
    "        self.all_chars = all_chars\n",
    "        self.decoder = dict(enumerate(all_chars))\n",
    "        self.encoder = {char:idx for idx, char in decoder.items()}\n",
    "\n",
    "        # use batch_first=True to match the format as batch generator\n",
    "        self.lstm = nn.LSTM(len(self.all_chars), num_hidden, num_layers, dropout=drop_prob, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc_linear = nn.Linear(num_hidden, len(self.all_chars))\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        lstm_output, hidden = self.lstm(x,hidden)\n",
    "        drop_output = self.dropout(lstm_output)\n",
    "        drop_output = drop_output.contiguous().view(-1,self.num_hidden) # reshaping\n",
    "        final_out = self.fc_linear(drop_output)\n",
    "\n",
    "        return final_out, hidden\n",
    "\n",
    "    def hidden_state(self, batch_size):\n",
    "        if self.use_gpu:\n",
    "            hidden = (torch.zeros(self.num_layers, batch_size, self.num_hidden).cuda(),\n",
    "                torch.zeros(self.num_layers, batch_size, self.num_hidden).cuda())\n",
    "        else:\n",
    "            hidden = (torch.zeros(self.num_layers, batch_size, self.num_hidden),\n",
    "                torch.zeros(self.num_layers, batch_size, self.num_hidden))\n",
    "\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "19d5a39b-d02e-47c2-9477-3261d7232d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharModel(all_chars=all_characters, num_hidden=512, num_layers=3, drop_prob=0.5, use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "56c2bcfb-fa0f-4ff2-837b-382798cc5c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_param = []\n",
    "for p in model.parameters():\n",
    "    total_param.append(int(p.numel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c2aa5c3c-856b-4134-9e5d-bc847d8beaf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5470292"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(total_param)\n",
    "# try to match size of dataset (at least order of magnitude)\n",
    "# too many parameters for too small text data -> overfitting\n",
    "# too less parameters for too much text data -> underfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "86035842-c62b-4982-a1d4-7292dda92a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "26217eed-ee7e-4e95-8c63-cbbd4e471ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_percent = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3164a765-e1ec-417b-ad75-9ba6a86fae07",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = int(len(encoded_text)*train_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5c0e330c-15a2-4ec6-a87b-db5e12545dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = encoded_text[:train_idx]\n",
    "val_data = encoded_text[train_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fe4964be-de43-41a4-a7a3-ede2ae34cb87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "544560"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "260f1b66-78a7-47be-bc58-941bdcda207a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4901049"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ec4b50d4-9578-44bb-966e-2ea90f3685b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_percent = 0.9 # majority of data will be used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d181f270-20ae-48d6-840c-4743b74c43a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = int(len(encoded_text) * train_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6debcbbb-8897-4cbf-86c4-c476f339e63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = encoded_text[:train_idx]\n",
    "val_data = encoded_text[train_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "394a19bf-7524-4e22-8dc1-626001e154a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables:\n",
    "epochs = 60\n",
    "batch_size = 100\n",
    "seq_len = 100\n",
    "\n",
    "tracker = 0\n",
    "num_char = max(encoded_text)+1 # indexing starting from 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "03d47949-3143-4093-a7a1-7e06fbb66715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 step: 75 val loss: 3.1928398609161377\n",
      "epoch: 0 step: 100 val loss: 3.1836488246917725\n",
      "epoch: 0 step: 125 val loss: 3.0538551807403564\n",
      "epoch: 0 step: 150 val loss: 2.960284471511841\n",
      "epoch: 0 step: 175 val loss: 2.842339515686035\n",
      "epoch: 0 step: 200 val loss: 2.726668357849121\n",
      "epoch: 0 step: 225 val loss: 2.6494829654693604\n",
      "epoch: 0 step: 250 val loss: 2.5526108741760254\n",
      "epoch: 0 step: 275 val loss: 2.429896593093872\n",
      "epoch: 0 step: 300 val loss: 2.3213629722595215\n",
      "epoch: 0 step: 325 val loss: 2.2449758052825928\n",
      "epoch: 0 step: 350 val loss: 2.186616897583008\n",
      "epoch: 0 step: 375 val loss: 2.1394622325897217\n",
      "epoch: 0 step: 400 val loss: 2.0962395668029785\n",
      "epoch: 0 step: 425 val loss: 2.0556678771972656\n",
      "epoch: 0 step: 450 val loss: 2.026746988296509\n",
      "epoch: 0 step: 475 val loss: 1.9988107681274414\n",
      "epoch: 0 step: 500 val loss: 1.9744154214859009\n",
      "epoch: 0 step: 525 val loss: 1.936339020729065\n",
      "epoch: 1 step: 550 val loss: 1.9136974811553955\n",
      "epoch: 1 step: 575 val loss: 1.890352487564087\n",
      "epoch: 1 step: 600 val loss: 1.8698318004608154\n",
      "epoch: 1 step: 625 val loss: 1.8429640531539917\n",
      "epoch: 1 step: 650 val loss: 1.8259388208389282\n",
      "epoch: 1 step: 675 val loss: 1.8105052709579468\n",
      "epoch: 1 step: 700 val loss: 1.7878096103668213\n",
      "epoch: 1 step: 725 val loss: 1.772719383239746\n",
      "epoch: 1 step: 750 val loss: 1.758471131324768\n",
      "epoch: 1 step: 775 val loss: 1.7473070621490479\n",
      "epoch: 1 step: 800 val loss: 1.7288521528244019\n",
      "epoch: 1 step: 825 val loss: 1.7198917865753174\n",
      "epoch: 1 step: 850 val loss: 1.7057405710220337\n",
      "epoch: 1 step: 875 val loss: 1.6976559162139893\n",
      "epoch: 1 step: 900 val loss: 1.6882734298706055\n",
      "epoch: 1 step: 925 val loss: 1.680462121963501\n",
      "epoch: 1 step: 950 val loss: 1.6697826385498047\n",
      "epoch: 1 step: 975 val loss: 1.6604821681976318\n",
      "epoch: 1 step: 1000 val loss: 1.6511183977127075\n",
      "epoch: 1 step: 1025 val loss: 1.63828706741333\n",
      "epoch: 2 step: 1050 val loss: 1.6379568576812744\n",
      "epoch: 2 step: 1075 val loss: 1.627418875694275\n",
      "epoch: 2 step: 1100 val loss: 1.6217877864837646\n",
      "epoch: 2 step: 1125 val loss: 1.610516905784607\n",
      "epoch: 2 step: 1150 val loss: 1.6054630279541016\n",
      "epoch: 2 step: 1175 val loss: 1.5965373516082764\n",
      "epoch: 2 step: 1200 val loss: 1.591085433959961\n",
      "epoch: 2 step: 1225 val loss: 1.5803191661834717\n",
      "epoch: 2 step: 1250 val loss: 1.57651948928833\n",
      "epoch: 2 step: 1275 val loss: 1.5714638233184814\n",
      "epoch: 2 step: 1300 val loss: 1.5645015239715576\n",
      "epoch: 2 step: 1325 val loss: 1.5628589391708374\n",
      "epoch: 2 step: 1350 val loss: 1.554916501045227\n",
      "epoch: 2 step: 1375 val loss: 1.5488141775131226\n",
      "epoch: 2 step: 1400 val loss: 1.5427883863449097\n",
      "epoch: 2 step: 1425 val loss: 1.541101098060608\n",
      "epoch: 2 step: 1450 val loss: 1.5357136726379395\n",
      "epoch: 2 step: 1475 val loss: 1.528764009475708\n",
      "epoch: 2 step: 1500 val loss: 1.5186476707458496\n",
      "epoch: 3 step: 1525 val loss: 1.5179154872894287\n",
      "epoch: 3 step: 1550 val loss: 1.5156160593032837\n",
      "epoch: 3 step: 1575 val loss: 1.5139179229736328\n",
      "epoch: 3 step: 1600 val loss: 1.5102709531784058\n",
      "epoch: 3 step: 1625 val loss: 1.504379153251648\n",
      "epoch: 3 step: 1650 val loss: 1.4961384534835815\n",
      "epoch: 3 step: 1675 val loss: 1.4924185276031494\n",
      "epoch: 3 step: 1700 val loss: 1.4883018732070923\n",
      "epoch: 3 step: 1725 val loss: 1.4846421480178833\n",
      "epoch: 3 step: 1750 val loss: 1.4788858890533447\n",
      "epoch: 3 step: 1775 val loss: 1.4760311841964722\n",
      "epoch: 3 step: 1800 val loss: 1.4743001461029053\n",
      "epoch: 3 step: 1825 val loss: 1.475609540939331\n",
      "epoch: 3 step: 1850 val loss: 1.4693955183029175\n",
      "epoch: 3 step: 1875 val loss: 1.4712557792663574\n",
      "epoch: 3 step: 1900 val loss: 1.464880347251892\n",
      "epoch: 3 step: 1925 val loss: 1.4665950536727905\n",
      "epoch: 3 step: 1950 val loss: 1.4556751251220703\n",
      "epoch: 3 step: 1975 val loss: 1.4592502117156982\n",
      "epoch: 3 step: 2000 val loss: 1.4480485916137695\n",
      "epoch: 4 step: 2025 val loss: 1.4534190893173218\n",
      "epoch: 4 step: 2050 val loss: 1.4480352401733398\n",
      "epoch: 4 step: 2075 val loss: 1.4424958229064941\n",
      "epoch: 4 step: 2100 val loss: 1.4408255815505981\n",
      "epoch: 4 step: 2125 val loss: 1.4421665668487549\n",
      "epoch: 4 step: 2150 val loss: 1.43601655960083\n",
      "epoch: 4 step: 2175 val loss: 1.433191180229187\n",
      "epoch: 4 step: 2200 val loss: 1.4317742586135864\n",
      "epoch: 4 step: 2225 val loss: 1.4278050661087036\n",
      "epoch: 4 step: 2250 val loss: 1.428815484046936\n",
      "epoch: 4 step: 2275 val loss: 1.4253292083740234\n",
      "epoch: 4 step: 2300 val loss: 1.4217075109481812\n",
      "epoch: 4 step: 2325 val loss: 1.4250917434692383\n",
      "epoch: 4 step: 2350 val loss: 1.4219841957092285\n",
      "epoch: 4 step: 2375 val loss: 1.4248218536376953\n",
      "epoch: 4 step: 2400 val loss: 1.4200857877731323\n",
      "epoch: 4 step: 2425 val loss: 1.4207890033721924\n",
      "epoch: 4 step: 2450 val loss: 1.4146087169647217\n",
      "epoch: 4 step: 2475 val loss: 1.4074724912643433\n",
      "epoch: 4 step: 2500 val loss: 1.4072619676589966\n",
      "epoch: 5 step: 2525 val loss: 1.404991626739502\n",
      "epoch: 5 step: 2550 val loss: 1.4079866409301758\n",
      "epoch: 5 step: 2575 val loss: 1.4028093814849854\n",
      "epoch: 5 step: 2600 val loss: 1.4046841859817505\n",
      "epoch: 5 step: 2625 val loss: 1.4047107696533203\n",
      "epoch: 5 step: 2650 val loss: 1.4022266864776611\n",
      "epoch: 5 step: 2675 val loss: 1.3998147249221802\n",
      "epoch: 5 step: 2700 val loss: 1.3923150300979614\n",
      "epoch: 5 step: 2725 val loss: 1.3896312713623047\n",
      "epoch: 5 step: 2750 val loss: 1.389531135559082\n",
      "epoch: 5 step: 2775 val loss: 1.3948345184326172\n",
      "epoch: 5 step: 2800 val loss: 1.394818902015686\n",
      "epoch: 5 step: 2825 val loss: 1.3863924741744995\n",
      "epoch: 5 step: 2850 val loss: 1.3927364349365234\n",
      "epoch: 5 step: 2875 val loss: 1.3928306102752686\n",
      "epoch: 5 step: 2900 val loss: 1.3928345441818237\n",
      "epoch: 5 step: 2925 val loss: 1.3969470262527466\n",
      "epoch: 5 step: 2950 val loss: 1.3846832513809204\n",
      "epoch: 5 step: 2975 val loss: 1.3819808959960938\n",
      "epoch: 6 step: 3000 val loss: 1.3778561353683472\n",
      "epoch: 6 step: 3025 val loss: 1.3824092149734497\n",
      "epoch: 6 step: 3050 val loss: 1.3763200044631958\n",
      "epoch: 6 step: 3075 val loss: 1.3756564855575562\n",
      "epoch: 6 step: 3100 val loss: 1.3762681484222412\n",
      "epoch: 6 step: 3125 val loss: 1.3785953521728516\n",
      "epoch: 6 step: 3150 val loss: 1.3754338026046753\n",
      "epoch: 6 step: 3175 val loss: 1.3733576536178589\n",
      "epoch: 6 step: 3200 val loss: 1.3692710399627686\n",
      "epoch: 6 step: 3225 val loss: 1.3677057027816772\n",
      "epoch: 6 step: 3250 val loss: 1.3673378229141235\n",
      "epoch: 6 step: 3275 val loss: 1.365054726600647\n",
      "epoch: 6 step: 3300 val loss: 1.3640035390853882\n",
      "epoch: 6 step: 3325 val loss: 1.3687304258346558\n",
      "epoch: 6 step: 3350 val loss: 1.372663140296936\n",
      "epoch: 6 step: 3375 val loss: 1.3755037784576416\n",
      "epoch: 6 step: 3400 val loss: 1.369520664215088\n",
      "epoch: 6 step: 3425 val loss: 1.3623038530349731\n",
      "epoch: 6 step: 3450 val loss: 1.3671249151229858\n",
      "epoch: 6 step: 3475 val loss: 1.356123447418213\n",
      "epoch: 7 step: 3500 val loss: 1.3630188703536987\n",
      "epoch: 7 step: 3525 val loss: 1.3646869659423828\n",
      "epoch: 7 step: 3550 val loss: 1.370071530342102\n",
      "epoch: 7 step: 3575 val loss: 1.3601465225219727\n",
      "epoch: 7 step: 3600 val loss: 1.359327793121338\n",
      "epoch: 7 step: 3625 val loss: 1.3609492778778076\n",
      "epoch: 7 step: 3650 val loss: 1.3548022508621216\n",
      "epoch: 7 step: 3675 val loss: 1.357865571975708\n",
      "epoch: 7 step: 3700 val loss: 1.3534941673278809\n",
      "epoch: 7 step: 3725 val loss: 1.354099154472351\n",
      "epoch: 7 step: 3750 val loss: 1.3535467386245728\n",
      "epoch: 7 step: 3775 val loss: 1.3503466844558716\n",
      "epoch: 7 step: 3800 val loss: 1.3523842096328735\n",
      "epoch: 7 step: 3825 val loss: 1.3559848070144653\n",
      "epoch: 7 step: 3850 val loss: 1.359411597251892\n",
      "epoch: 7 step: 3875 val loss: 1.3616282939910889\n",
      "epoch: 7 step: 3900 val loss: 1.354443907737732\n",
      "epoch: 7 step: 3925 val loss: 1.3542866706848145\n",
      "epoch: 7 step: 3950 val loss: 1.353554368019104\n",
      "epoch: 8 step: 3975 val loss: 1.3504287004470825\n",
      "epoch: 8 step: 4000 val loss: 1.3565781116485596\n",
      "epoch: 8 step: 4025 val loss: 1.3530464172363281\n",
      "epoch: 8 step: 4050 val loss: 1.3563008308410645\n",
      "epoch: 8 step: 4075 val loss: 1.3584322929382324\n",
      "epoch: 8 step: 4100 val loss: 1.3474311828613281\n",
      "epoch: 8 step: 4125 val loss: 1.3497015237808228\n",
      "epoch: 8 step: 4150 val loss: 1.3484846353530884\n",
      "epoch: 8 step: 4175 val loss: 1.344946026802063\n",
      "epoch: 8 step: 4200 val loss: 1.3383132219314575\n",
      "epoch: 8 step: 4225 val loss: 1.348182201385498\n",
      "epoch: 8 step: 4250 val loss: 1.3484325408935547\n",
      "epoch: 8 step: 4275 val loss: 1.3463220596313477\n",
      "epoch: 8 step: 4300 val loss: 1.3372703790664673\n",
      "epoch: 8 step: 4325 val loss: 1.3506243228912354\n",
      "epoch: 8 step: 4350 val loss: 1.34598970413208\n",
      "epoch: 8 step: 4375 val loss: 1.3432053327560425\n",
      "epoch: 8 step: 4400 val loss: 1.3439092636108398\n",
      "epoch: 8 step: 4425 val loss: 1.3456707000732422\n",
      "epoch: 8 step: 4450 val loss: 1.337428331375122\n",
      "epoch: 9 step: 4475 val loss: 1.3423961400985718\n",
      "epoch: 9 step: 4500 val loss: 1.341487169265747\n",
      "epoch: 9 step: 4525 val loss: 1.3372924327850342\n",
      "epoch: 9 step: 4550 val loss: 1.336674690246582\n",
      "epoch: 9 step: 4575 val loss: 1.3378124237060547\n",
      "epoch: 9 step: 4600 val loss: 1.3357841968536377\n",
      "epoch: 9 step: 4625 val loss: 1.3367470502853394\n",
      "epoch: 9 step: 4650 val loss: 1.337188720703125\n",
      "epoch: 9 step: 4675 val loss: 1.334601640701294\n",
      "epoch: 9 step: 4700 val loss: 1.3353358507156372\n",
      "epoch: 9 step: 4725 val loss: 1.3358980417251587\n",
      "epoch: 9 step: 4750 val loss: 1.334131121635437\n",
      "epoch: 9 step: 4775 val loss: 1.329750895500183\n",
      "epoch: 9 step: 4800 val loss: 1.3340599536895752\n",
      "epoch: 9 step: 4825 val loss: 1.3394049406051636\n",
      "epoch: 9 step: 4850 val loss: 1.3425837755203247\n",
      "epoch: 9 step: 4875 val loss: 1.3414757251739502\n",
      "epoch: 9 step: 4900 val loss: 1.3368029594421387\n",
      "epoch: 9 step: 4925 val loss: 1.3362810611724854\n",
      "epoch: 9 step: 4950 val loss: 1.3351327180862427\n",
      "epoch: 10 step: 4975 val loss: 1.3375422954559326\n",
      "epoch: 10 step: 5000 val loss: 1.3369373083114624\n",
      "epoch: 10 step: 5025 val loss: 1.3390095233917236\n",
      "epoch: 10 step: 5050 val loss: 1.3360310792922974\n",
      "epoch: 10 step: 5075 val loss: 1.3323601484298706\n",
      "epoch: 10 step: 5100 val loss: 1.3294775485992432\n",
      "epoch: 10 step: 5125 val loss: 1.3330755233764648\n",
      "epoch: 10 step: 5150 val loss: 1.3282314538955688\n",
      "epoch: 10 step: 5175 val loss: 1.3259551525115967\n",
      "epoch: 10 step: 5200 val loss: 1.3246673345565796\n",
      "epoch: 10 step: 5225 val loss: 1.3294758796691895\n",
      "epoch: 10 step: 5250 val loss: 1.3310890197753906\n",
      "epoch: 10 step: 5275 val loss: 1.3250172138214111\n",
      "epoch: 10 step: 5300 val loss: 1.3346450328826904\n",
      "epoch: 10 step: 5325 val loss: 1.33109450340271\n",
      "epoch: 10 step: 5350 val loss: 1.3342994451522827\n",
      "epoch: 10 step: 5375 val loss: 1.3332446813583374\n",
      "epoch: 10 step: 5400 val loss: 1.3314416408538818\n",
      "epoch: 10 step: 5425 val loss: 1.3315095901489258\n",
      "epoch: 11 step: 5450 val loss: 1.3275389671325684\n",
      "epoch: 11 step: 5475 val loss: 1.332620620727539\n",
      "epoch: 11 step: 5500 val loss: 1.3309398889541626\n",
      "epoch: 11 step: 5525 val loss: 1.325893759727478\n",
      "epoch: 11 step: 5550 val loss: 1.3327298164367676\n",
      "epoch: 11 step: 5575 val loss: 1.3251686096191406\n",
      "epoch: 11 step: 5600 val loss: 1.3238474130630493\n",
      "epoch: 11 step: 5625 val loss: 1.326568841934204\n",
      "epoch: 11 step: 5650 val loss: 1.321691632270813\n",
      "epoch: 11 step: 5675 val loss: 1.3195518255233765\n",
      "epoch: 11 step: 5700 val loss: 1.3257038593292236\n",
      "epoch: 11 step: 5725 val loss: 1.3226021528244019\n",
      "epoch: 11 step: 5750 val loss: 1.317168116569519\n",
      "epoch: 11 step: 5775 val loss: 1.3166922330856323\n",
      "epoch: 11 step: 5800 val loss: 1.3288015127182007\n",
      "epoch: 11 step: 5825 val loss: 1.3367047309875488\n",
      "epoch: 11 step: 5850 val loss: 1.338525652885437\n",
      "epoch: 11 step: 5875 val loss: 1.3244134187698364\n",
      "epoch: 11 step: 5900 val loss: 1.3289403915405273\n",
      "epoch: 11 step: 5925 val loss: 1.325109601020813\n",
      "epoch: 12 step: 5950 val loss: 1.324992060661316\n",
      "epoch: 12 step: 5975 val loss: 1.3325161933898926\n",
      "epoch: 12 step: 6000 val loss: 1.3324719667434692\n",
      "epoch: 12 step: 6025 val loss: 1.3245880603790283\n",
      "epoch: 12 step: 6050 val loss: 1.3238134384155273\n",
      "epoch: 12 step: 6075 val loss: 1.3240405321121216\n",
      "epoch: 12 step: 6100 val loss: 1.3171318769454956\n",
      "epoch: 12 step: 6125 val loss: 1.3186516761779785\n",
      "epoch: 12 step: 6150 val loss: 1.323972463607788\n",
      "epoch: 12 step: 6175 val loss: 1.312503695487976\n",
      "epoch: 12 step: 6200 val loss: 1.3175169229507446\n",
      "epoch: 12 step: 6225 val loss: 1.3193230628967285\n",
      "epoch: 12 step: 6250 val loss: 1.3167074918746948\n",
      "epoch: 12 step: 6275 val loss: 1.3153642416000366\n",
      "epoch: 12 step: 6300 val loss: 1.3178967237472534\n",
      "epoch: 12 step: 6325 val loss: 1.3240622282028198\n",
      "epoch: 12 step: 6350 val loss: 1.3242201805114746\n",
      "epoch: 12 step: 6375 val loss: 1.314322590827942\n",
      "epoch: 12 step: 6400 val loss: 1.3161015510559082\n",
      "epoch: 13 step: 6425 val loss: 1.3208478689193726\n",
      "epoch: 13 step: 6450 val loss: 1.321213722229004\n",
      "epoch: 13 step: 6475 val loss: 1.3207427263259888\n",
      "epoch: 13 step: 6500 val loss: 1.3199574947357178\n",
      "epoch: 13 step: 6525 val loss: 1.3177012205123901\n",
      "epoch: 13 step: 6550 val loss: 1.3157739639282227\n",
      "epoch: 13 step: 6575 val loss: 1.314421534538269\n",
      "epoch: 13 step: 6600 val loss: 1.315921664237976\n",
      "epoch: 13 step: 6625 val loss: 1.316009283065796\n",
      "epoch: 13 step: 6650 val loss: 1.314420223236084\n",
      "epoch: 13 step: 6675 val loss: 1.3148317337036133\n",
      "epoch: 13 step: 6700 val loss: 1.3183832168579102\n",
      "epoch: 13 step: 6725 val loss: 1.3125011920928955\n",
      "epoch: 13 step: 6750 val loss: 1.3120733499526978\n",
      "epoch: 13 step: 6775 val loss: 1.3165990114212036\n",
      "epoch: 13 step: 6800 val loss: 1.3192193508148193\n",
      "epoch: 13 step: 6825 val loss: 1.3241543769836426\n",
      "epoch: 13 step: 6850 val loss: 1.3213045597076416\n",
      "epoch: 13 step: 6875 val loss: 1.3203637599945068\n",
      "epoch: 13 step: 6900 val loss: 1.3177083730697632\n",
      "epoch: 14 step: 6925 val loss: 1.3138575553894043\n",
      "epoch: 14 step: 6950 val loss: 1.3150172233581543\n",
      "epoch: 14 step: 6975 val loss: 1.3139688968658447\n",
      "epoch: 14 step: 7000 val loss: 1.3117228746414185\n",
      "epoch: 14 step: 7025 val loss: 1.3165931701660156\n",
      "epoch: 14 step: 7050 val loss: 1.315093994140625\n",
      "epoch: 14 step: 7075 val loss: 1.3122862577438354\n",
      "epoch: 14 step: 7100 val loss: 1.3109021186828613\n",
      "epoch: 14 step: 7125 val loss: 1.316160798072815\n",
      "epoch: 14 step: 7150 val loss: 1.3128615617752075\n",
      "epoch: 14 step: 7175 val loss: 1.3113737106323242\n",
      "epoch: 14 step: 7200 val loss: 1.31293523311615\n",
      "epoch: 14 step: 7225 val loss: 1.3078219890594482\n",
      "epoch: 14 step: 7250 val loss: 1.310915470123291\n",
      "epoch: 14 step: 7275 val loss: 1.313332200050354\n",
      "epoch: 14 step: 7300 val loss: 1.3211159706115723\n",
      "epoch: 14 step: 7325 val loss: 1.3200464248657227\n",
      "epoch: 14 step: 7350 val loss: 1.3162447214126587\n",
      "epoch: 14 step: 7375 val loss: 1.3181560039520264\n",
      "epoch: 14 step: 7400 val loss: 1.3156245946884155\n",
      "epoch: 15 step: 7425 val loss: 1.315183401107788\n",
      "epoch: 15 step: 7450 val loss: 1.3194141387939453\n",
      "epoch: 15 step: 7475 val loss: 1.3206851482391357\n",
      "epoch: 15 step: 7500 val loss: 1.3132082223892212\n",
      "epoch: 15 step: 7525 val loss: 1.3139538764953613\n",
      "epoch: 15 step: 7550 val loss: 1.3117883205413818\n",
      "epoch: 15 step: 7575 val loss: 1.3073904514312744\n",
      "epoch: 15 step: 7600 val loss: 1.3127508163452148\n",
      "epoch: 15 step: 7625 val loss: 1.3103904724121094\n",
      "epoch: 15 step: 7650 val loss: 1.305766224861145\n",
      "epoch: 15 step: 7675 val loss: 1.3145501613616943\n",
      "epoch: 15 step: 7700 val loss: 1.3135490417480469\n",
      "epoch: 15 step: 7725 val loss: 1.306917428970337\n",
      "epoch: 15 step: 7750 val loss: 1.3136250972747803\n",
      "epoch: 15 step: 7775 val loss: 1.3073629140853882\n",
      "epoch: 15 step: 7800 val loss: 1.315476655960083\n",
      "epoch: 15 step: 7825 val loss: 1.3164016008377075\n",
      "epoch: 15 step: 7850 val loss: 1.3127222061157227\n",
      "epoch: 15 step: 7875 val loss: 1.3137645721435547\n",
      "epoch: 16 step: 7900 val loss: 1.3112752437591553\n",
      "epoch: 16 step: 7925 val loss: 1.3112996816635132\n",
      "epoch: 16 step: 7950 val loss: 1.3076415061950684\n",
      "epoch: 16 step: 7975 val loss: 1.3089666366577148\n",
      "epoch: 16 step: 8000 val loss: 1.3088722229003906\n",
      "epoch: 16 step: 8025 val loss: 1.3082019090652466\n",
      "epoch: 16 step: 8050 val loss: 1.3099032640457153\n",
      "epoch: 16 step: 8075 val loss: 1.3072524070739746\n",
      "epoch: 16 step: 8100 val loss: 1.3079758882522583\n",
      "epoch: 16 step: 8125 val loss: 1.3077572584152222\n",
      "epoch: 16 step: 8150 val loss: 1.305221438407898\n",
      "epoch: 16 step: 8175 val loss: 1.3073714971542358\n",
      "epoch: 16 step: 8200 val loss: 1.3030036687850952\n",
      "epoch: 16 step: 8225 val loss: 1.3029371500015259\n",
      "epoch: 16 step: 8250 val loss: 1.307286024093628\n",
      "epoch: 16 step: 8275 val loss: 1.3130254745483398\n",
      "epoch: 16 step: 8300 val loss: 1.3173338174819946\n",
      "epoch: 16 step: 8325 val loss: 1.3112677335739136\n",
      "epoch: 16 step: 8350 val loss: 1.3121130466461182\n",
      "epoch: 16 step: 8375 val loss: 1.3061065673828125\n",
      "epoch: 17 step: 8400 val loss: 1.3043392896652222\n",
      "epoch: 17 step: 8425 val loss: 1.3038275241851807\n",
      "epoch: 17 step: 8450 val loss: 1.3085745573043823\n",
      "epoch: 17 step: 8475 val loss: 1.2987260818481445\n",
      "epoch: 17 step: 8500 val loss: 1.3011454343795776\n",
      "epoch: 17 step: 8525 val loss: 1.2995753288269043\n",
      "epoch: 17 step: 8550 val loss: 1.3045105934143066\n",
      "epoch: 17 step: 8575 val loss: 1.302403450012207\n",
      "epoch: 17 step: 8600 val loss: 1.3107134103775024\n",
      "epoch: 17 step: 8625 val loss: 1.3022812604904175\n",
      "epoch: 17 step: 8650 val loss: 1.3034474849700928\n",
      "epoch: 17 step: 8675 val loss: 1.306958556175232\n",
      "epoch: 17 step: 8700 val loss: 1.3105347156524658\n",
      "epoch: 17 step: 8725 val loss: 1.3080369234085083\n",
      "epoch: 17 step: 8750 val loss: 1.3095505237579346\n",
      "epoch: 17 step: 8775 val loss: 1.314023733139038\n",
      "epoch: 17 step: 8800 val loss: 1.3101426362991333\n",
      "epoch: 17 step: 8825 val loss: 1.305808186531067\n",
      "epoch: 17 step: 8850 val loss: 1.3067106008529663\n",
      "epoch: 18 step: 8875 val loss: 1.3097306489944458\n",
      "epoch: 18 step: 8900 val loss: 1.307321310043335\n",
      "epoch: 18 step: 8925 val loss: 1.3110812902450562\n",
      "epoch: 18 step: 8950 val loss: 1.3087879419326782\n",
      "epoch: 18 step: 8975 val loss: 1.3116651773452759\n",
      "epoch: 18 step: 9000 val loss: 1.3055951595306396\n",
      "epoch: 18 step: 9025 val loss: 1.3059009313583374\n",
      "epoch: 18 step: 9050 val loss: 1.3074148893356323\n",
      "epoch: 18 step: 9075 val loss: 1.30198335647583\n",
      "epoch: 18 step: 9100 val loss: 1.3031026124954224\n",
      "epoch: 18 step: 9125 val loss: 1.2950868606567383\n",
      "epoch: 18 step: 9150 val loss: 1.3020728826522827\n",
      "epoch: 18 step: 9175 val loss: 1.3045474290847778\n",
      "epoch: 18 step: 9200 val loss: 1.3026704788208008\n",
      "epoch: 18 step: 9225 val loss: 1.3064743280410767\n",
      "epoch: 18 step: 9250 val loss: 1.3075125217437744\n",
      "epoch: 18 step: 9275 val loss: 1.3086659908294678\n",
      "epoch: 18 step: 9300 val loss: 1.313162922859192\n",
      "epoch: 18 step: 9325 val loss: 1.3065906763076782\n",
      "epoch: 18 step: 9350 val loss: 1.3047715425491333\n",
      "epoch: 19 step: 9375 val loss: 1.3078418970108032\n",
      "epoch: 19 step: 9400 val loss: 1.3116213083267212\n",
      "epoch: 19 step: 9425 val loss: 1.3080246448516846\n",
      "epoch: 19 step: 9450 val loss: 1.303925633430481\n",
      "epoch: 19 step: 9475 val loss: 1.306932806968689\n",
      "epoch: 19 step: 9500 val loss: 1.3057864904403687\n",
      "epoch: 19 step: 9525 val loss: 1.2988706827163696\n",
      "epoch: 19 step: 9550 val loss: 1.3012983798980713\n",
      "epoch: 19 step: 9575 val loss: 1.297136664390564\n",
      "epoch: 19 step: 9600 val loss: 1.3023755550384521\n",
      "epoch: 19 step: 9625 val loss: 1.2972909212112427\n",
      "epoch: 19 step: 9650 val loss: 1.3010332584381104\n",
      "epoch: 19 step: 9675 val loss: 1.307576060295105\n",
      "epoch: 19 step: 9700 val loss: 1.3071653842926025\n",
      "epoch: 19 step: 9725 val loss: 1.3048913478851318\n",
      "epoch: 19 step: 9750 val loss: 1.3104315996170044\n",
      "epoch: 19 step: 9775 val loss: 1.3044689893722534\n",
      "epoch: 19 step: 9800 val loss: 1.3024026155471802\n",
      "epoch: 19 step: 9825 val loss: 1.3065577745437622\n",
      "epoch: 19 step: 9850 val loss: 1.30427885055542\n",
      "epoch: 20 step: 9875 val loss: 1.3066025972366333\n",
      "epoch: 20 step: 9900 val loss: 1.308219313621521\n",
      "epoch: 20 step: 9925 val loss: 1.3098478317260742\n",
      "epoch: 20 step: 9950 val loss: 1.3064130544662476\n",
      "epoch: 20 step: 9975 val loss: 1.302261471748352\n",
      "epoch: 20 step: 10000 val loss: 1.3039228916168213\n",
      "epoch: 20 step: 10025 val loss: 1.3037794828414917\n",
      "epoch: 20 step: 10050 val loss: 1.3040844202041626\n",
      "epoch: 20 step: 10075 val loss: 1.3027108907699585\n",
      "epoch: 20 step: 10100 val loss: 1.3021669387817383\n",
      "epoch: 20 step: 10125 val loss: 1.304740309715271\n",
      "epoch: 20 step: 10150 val loss: 1.301895022392273\n",
      "epoch: 20 step: 10175 val loss: 1.299852967262268\n",
      "epoch: 20 step: 10200 val loss: 1.3043391704559326\n",
      "epoch: 20 step: 10225 val loss: 1.3031654357910156\n",
      "epoch: 20 step: 10250 val loss: 1.3051881790161133\n",
      "epoch: 20 step: 10275 val loss: 1.3062175512313843\n",
      "epoch: 20 step: 10300 val loss: 1.299586534500122\n",
      "epoch: 20 step: 10325 val loss: 1.301462173461914\n",
      "epoch: 21 step: 10350 val loss: 1.3061578273773193\n",
      "epoch: 21 step: 10375 val loss: 1.3063552379608154\n",
      "epoch: 21 step: 10400 val loss: 1.3137145042419434\n",
      "epoch: 21 step: 10425 val loss: 1.3051587343215942\n",
      "epoch: 21 step: 10450 val loss: 1.3076611757278442\n",
      "epoch: 21 step: 10475 val loss: 1.303571343421936\n",
      "epoch: 21 step: 10500 val loss: 1.3078988790512085\n",
      "epoch: 21 step: 10525 val loss: 1.3015390634536743\n",
      "epoch: 21 step: 10550 val loss: 1.300053596496582\n",
      "epoch: 21 step: 10575 val loss: 1.3027489185333252\n",
      "epoch: 21 step: 10600 val loss: 1.2955145835876465\n",
      "epoch: 21 step: 10625 val loss: 1.2973196506500244\n",
      "epoch: 21 step: 10650 val loss: 1.3007831573486328\n",
      "epoch: 21 step: 10675 val loss: 1.296817421913147\n",
      "epoch: 21 step: 10700 val loss: 1.3040966987609863\n",
      "epoch: 21 step: 10725 val loss: 1.3057585954666138\n",
      "epoch: 21 step: 10750 val loss: 1.307752251625061\n",
      "epoch: 21 step: 10775 val loss: 1.3086192607879639\n",
      "epoch: 21 step: 10800 val loss: 1.308892011642456\n",
      "epoch: 21 step: 10825 val loss: 1.3049261569976807\n",
      "epoch: 22 step: 10850 val loss: 1.3069908618927002\n",
      "epoch: 22 step: 10875 val loss: 1.3092875480651855\n",
      "epoch: 22 step: 10900 val loss: 1.3094717264175415\n",
      "epoch: 22 step: 10925 val loss: 1.3026325702667236\n",
      "epoch: 22 step: 10950 val loss: 1.3072630167007446\n",
      "epoch: 22 step: 10975 val loss: 1.3045711517333984\n",
      "epoch: 22 step: 11000 val loss: 1.3009073734283447\n",
      "epoch: 22 step: 11025 val loss: 1.3016093969345093\n",
      "epoch: 22 step: 11050 val loss: 1.3032960891723633\n",
      "epoch: 22 step: 11075 val loss: 1.2977681159973145\n",
      "epoch: 22 step: 11100 val loss: 1.304567813873291\n",
      "epoch: 22 step: 11125 val loss: 1.301651120185852\n",
      "epoch: 22 step: 11150 val loss: 1.3157267570495605\n",
      "epoch: 22 step: 11175 val loss: 1.297418475151062\n",
      "epoch: 22 step: 11200 val loss: 1.298553466796875\n",
      "epoch: 22 step: 11225 val loss: 1.3040175437927246\n",
      "epoch: 22 step: 11250 val loss: 1.3077774047851562\n",
      "epoch: 22 step: 11275 val loss: 1.302842617034912\n",
      "epoch: 22 step: 11300 val loss: 1.3046104907989502\n",
      "epoch: 23 step: 11325 val loss: 1.3031362295150757\n",
      "epoch: 23 step: 11350 val loss: 1.30594801902771\n",
      "epoch: 23 step: 11375 val loss: 1.304490089416504\n",
      "epoch: 23 step: 11400 val loss: 1.3023884296417236\n",
      "epoch: 23 step: 11425 val loss: 1.3049876689910889\n",
      "epoch: 23 step: 11450 val loss: 1.3062573671340942\n",
      "epoch: 23 step: 11475 val loss: 1.306472897529602\n",
      "epoch: 23 step: 11500 val loss: 1.3084568977355957\n",
      "epoch: 23 step: 11525 val loss: 1.3040481805801392\n",
      "epoch: 23 step: 11550 val loss: 1.303987741470337\n",
      "epoch: 23 step: 11575 val loss: 1.3015376329421997\n",
      "epoch: 23 step: 11600 val loss: 1.3032344579696655\n",
      "epoch: 23 step: 11625 val loss: 1.3047415018081665\n",
      "epoch: 23 step: 11650 val loss: 1.3012278079986572\n",
      "epoch: 23 step: 11675 val loss: 1.305931806564331\n",
      "epoch: 23 step: 11700 val loss: 1.3073917627334595\n",
      "epoch: 23 step: 11725 val loss: 1.3054800033569336\n",
      "epoch: 23 step: 11750 val loss: 1.3118573427200317\n",
      "epoch: 23 step: 11775 val loss: 1.311422348022461\n",
      "epoch: 23 step: 11800 val loss: 1.3043220043182373\n",
      "epoch: 24 step: 11825 val loss: 1.3053990602493286\n",
      "epoch: 24 step: 11850 val loss: 1.3062704801559448\n",
      "epoch: 24 step: 11875 val loss: 1.3057245016098022\n",
      "epoch: 24 step: 11900 val loss: 1.2993919849395752\n",
      "epoch: 24 step: 11925 val loss: 1.3068164587020874\n",
      "epoch: 24 step: 11950 val loss: 1.302207112312317\n",
      "epoch: 24 step: 11975 val loss: 1.3004326820373535\n",
      "epoch: 24 step: 12000 val loss: 1.3014177083969116\n",
      "epoch: 24 step: 12025 val loss: 1.2988224029541016\n",
      "epoch: 24 step: 12050 val loss: 1.3010046482086182\n",
      "epoch: 24 step: 12075 val loss: 1.297827124595642\n",
      "epoch: 24 step: 12100 val loss: 1.2977842092514038\n",
      "epoch: 24 step: 12125 val loss: 1.2957022190093994\n",
      "epoch: 24 step: 12150 val loss: 1.2957420349121094\n",
      "epoch: 24 step: 12175 val loss: 1.3009662628173828\n",
      "epoch: 24 step: 12200 val loss: 1.30379056930542\n",
      "epoch: 24 step: 12225 val loss: 1.3045810461044312\n",
      "epoch: 24 step: 12250 val loss: 1.305626630783081\n",
      "epoch: 24 step: 12275 val loss: 1.305010199546814\n",
      "epoch: 24 step: 12300 val loss: 1.3014417886734009\n",
      "epoch: 25 step: 12325 val loss: 1.3027805089950562\n",
      "epoch: 25 step: 12350 val loss: 1.3032501935958862\n",
      "epoch: 25 step: 12375 val loss: 1.3015702962875366\n",
      "epoch: 25 step: 12400 val loss: 1.3023054599761963\n",
      "epoch: 25 step: 12425 val loss: 1.3002386093139648\n",
      "epoch: 25 step: 12450 val loss: 1.298861026763916\n",
      "epoch: 25 step: 12475 val loss: 1.2964198589324951\n",
      "epoch: 25 step: 12500 val loss: 1.2970272302627563\n",
      "epoch: 25 step: 12525 val loss: 1.2991197109222412\n",
      "epoch: 25 step: 12550 val loss: 1.2970620393753052\n",
      "epoch: 25 step: 12575 val loss: 1.2963659763336182\n",
      "epoch: 25 step: 12600 val loss: 1.2986397743225098\n",
      "epoch: 25 step: 12625 val loss: 1.2964569330215454\n",
      "epoch: 25 step: 12650 val loss: 1.2988674640655518\n",
      "epoch: 25 step: 12675 val loss: 1.2958855628967285\n",
      "epoch: 25 step: 12700 val loss: 1.301735520362854\n",
      "epoch: 25 step: 12725 val loss: 1.309325098991394\n",
      "epoch: 25 step: 12750 val loss: 1.305059790611267\n",
      "epoch: 25 step: 12775 val loss: 1.3049156665802002\n",
      "epoch: 26 step: 12800 val loss: 1.3034595251083374\n",
      "epoch: 26 step: 12825 val loss: 1.303654670715332\n",
      "epoch: 26 step: 12850 val loss: 1.3056243658065796\n",
      "epoch: 26 step: 12875 val loss: 1.302804708480835\n",
      "epoch: 26 step: 12900 val loss: 1.3009727001190186\n",
      "epoch: 26 step: 12925 val loss: 1.3013403415679932\n",
      "epoch: 26 step: 12950 val loss: 1.3049196004867554\n",
      "epoch: 26 step: 12975 val loss: 1.3000723123550415\n",
      "epoch: 26 step: 13000 val loss: 1.2961076498031616\n",
      "epoch: 26 step: 13025 val loss: 1.2927178144454956\n",
      "epoch: 26 step: 13050 val loss: 1.293162226676941\n",
      "epoch: 26 step: 13075 val loss: 1.2957626581192017\n",
      "epoch: 26 step: 13100 val loss: 1.297285795211792\n",
      "epoch: 26 step: 13125 val loss: 1.2990784645080566\n",
      "epoch: 26 step: 13150 val loss: 1.302985429763794\n",
      "epoch: 26 step: 13175 val loss: 1.3048428297042847\n",
      "epoch: 26 step: 13200 val loss: 1.3122788667678833\n",
      "epoch: 26 step: 13225 val loss: 1.3102900981903076\n",
      "epoch: 26 step: 13250 val loss: 1.3053431510925293\n",
      "epoch: 26 step: 13275 val loss: 1.304908275604248\n",
      "epoch: 27 step: 13300 val loss: 1.3023600578308105\n",
      "epoch: 27 step: 13325 val loss: 1.304628849029541\n",
      "epoch: 27 step: 13350 val loss: 1.30584716796875\n",
      "epoch: 27 step: 13375 val loss: 1.3001834154129028\n",
      "epoch: 27 step: 13400 val loss: 1.3029230833053589\n",
      "epoch: 27 step: 13425 val loss: 1.303093671798706\n",
      "epoch: 27 step: 13450 val loss: 1.3005890846252441\n",
      "epoch: 27 step: 13475 val loss: 1.3008049726486206\n",
      "epoch: 27 step: 13500 val loss: 1.2971570491790771\n",
      "epoch: 27 step: 13525 val loss: 1.2966179847717285\n",
      "epoch: 27 step: 13550 val loss: 1.298092007637024\n",
      "epoch: 27 step: 13575 val loss: 1.2987470626831055\n",
      "epoch: 27 step: 13600 val loss: 1.2980923652648926\n",
      "epoch: 27 step: 13625 val loss: 1.300486445426941\n",
      "epoch: 27 step: 13650 val loss: 1.3027490377426147\n",
      "epoch: 27 step: 13675 val loss: 1.3070085048675537\n",
      "epoch: 27 step: 13700 val loss: 1.307218313217163\n",
      "epoch: 27 step: 13725 val loss: 1.3088445663452148\n",
      "epoch: 27 step: 13750 val loss: 1.3071136474609375\n",
      "epoch: 28 step: 13775 val loss: 1.3072702884674072\n",
      "epoch: 28 step: 13800 val loss: 1.3074052333831787\n",
      "epoch: 28 step: 13825 val loss: 1.3092920780181885\n",
      "epoch: 28 step: 13850 val loss: 1.3050055503845215\n",
      "epoch: 28 step: 13875 val loss: 1.305831789970398\n",
      "epoch: 28 step: 13900 val loss: 1.2997968196868896\n",
      "epoch: 28 step: 13925 val loss: 1.29990816116333\n",
      "epoch: 28 step: 13950 val loss: 1.3007252216339111\n",
      "epoch: 28 step: 13975 val loss: 1.302657961845398\n",
      "epoch: 28 step: 14000 val loss: 1.303666591644287\n",
      "epoch: 28 step: 14025 val loss: 1.2980859279632568\n",
      "epoch: 28 step: 14050 val loss: 1.3023563623428345\n",
      "epoch: 28 step: 14075 val loss: 1.3034602403640747\n",
      "epoch: 28 step: 14100 val loss: 1.2987157106399536\n",
      "epoch: 28 step: 14125 val loss: 1.308592677116394\n",
      "epoch: 28 step: 14150 val loss: 1.3013285398483276\n",
      "epoch: 28 step: 14175 val loss: 1.3048503398895264\n",
      "epoch: 28 step: 14200 val loss: 1.3088035583496094\n",
      "epoch: 28 step: 14225 val loss: 1.3092193603515625\n",
      "epoch: 28 step: 14250 val loss: 1.3070365190505981\n",
      "epoch: 29 step: 14275 val loss: 1.3087544441223145\n",
      "epoch: 29 step: 14300 val loss: 1.3068132400512695\n",
      "epoch: 29 step: 14325 val loss: 1.3078162670135498\n",
      "epoch: 29 step: 14350 val loss: 1.3041030168533325\n",
      "epoch: 29 step: 14375 val loss: 1.303529143333435\n",
      "epoch: 29 step: 14400 val loss: 1.3021334409713745\n",
      "epoch: 29 step: 14425 val loss: 1.3003603219985962\n",
      "epoch: 29 step: 14450 val loss: 1.3020843267440796\n",
      "epoch: 29 step: 14475 val loss: 1.2973971366882324\n",
      "epoch: 29 step: 14500 val loss: 1.2949824333190918\n",
      "epoch: 29 step: 14525 val loss: 1.2936992645263672\n",
      "epoch: 29 step: 14550 val loss: 1.294182300567627\n",
      "epoch: 29 step: 14575 val loss: 1.2911795377731323\n",
      "epoch: 29 step: 14600 val loss: 1.295114517211914\n",
      "epoch: 29 step: 14625 val loss: 1.2980068922042847\n",
      "epoch: 29 step: 14650 val loss: 1.3024749755859375\n",
      "epoch: 29 step: 14675 val loss: 1.3012195825576782\n",
      "epoch: 29 step: 14700 val loss: 1.303365707397461\n",
      "epoch: 29 step: 14725 val loss: 1.3012075424194336\n",
      "epoch: 29 step: 14750 val loss: 1.2994266748428345\n",
      "epoch: 30 step: 14775 val loss: 1.2997307777404785\n",
      "epoch: 30 step: 14800 val loss: 1.2994154691696167\n",
      "epoch: 30 step: 14825 val loss: 1.2952721118927002\n",
      "epoch: 30 step: 14850 val loss: 1.2956457138061523\n",
      "epoch: 30 step: 14875 val loss: 1.2968860864639282\n",
      "epoch: 30 step: 14900 val loss: 1.2985771894454956\n",
      "epoch: 30 step: 14925 val loss: 1.2953269481658936\n",
      "epoch: 30 step: 14950 val loss: 1.297575831413269\n",
      "epoch: 30 step: 14975 val loss: 1.2924007177352905\n",
      "epoch: 30 step: 15000 val loss: 1.2874860763549805\n",
      "epoch: 30 step: 15025 val loss: 1.2949767112731934\n",
      "epoch: 30 step: 15050 val loss: 1.2903777360916138\n",
      "epoch: 30 step: 15075 val loss: 1.2928767204284668\n",
      "epoch: 30 step: 15100 val loss: 1.30284583568573\n",
      "epoch: 30 step: 15125 val loss: 1.301128625869751\n",
      "epoch: 30 step: 15150 val loss: 1.3109184503555298\n",
      "epoch: 30 step: 15175 val loss: 1.307856798171997\n",
      "epoch: 30 step: 15200 val loss: 1.301279902458191\n",
      "epoch: 30 step: 15225 val loss: 1.2988122701644897\n",
      "epoch: 31 step: 15250 val loss: 1.3011047840118408\n",
      "epoch: 31 step: 15275 val loss: 1.3022507429122925\n",
      "epoch: 31 step: 15300 val loss: 1.3057138919830322\n",
      "epoch: 31 step: 15325 val loss: 1.2977017164230347\n",
      "epoch: 31 step: 15350 val loss: 1.2991427183151245\n",
      "epoch: 31 step: 15375 val loss: 1.2975568771362305\n",
      "epoch: 31 step: 15400 val loss: 1.3020765781402588\n",
      "epoch: 31 step: 15425 val loss: 1.2963522672653198\n",
      "epoch: 31 step: 15450 val loss: 1.2925554513931274\n",
      "epoch: 31 step: 15475 val loss: 1.2939802408218384\n",
      "epoch: 31 step: 15500 val loss: 1.291934609413147\n",
      "epoch: 31 step: 15525 val loss: 1.2940422296524048\n",
      "epoch: 31 step: 15550 val loss: 1.2955771684646606\n",
      "epoch: 31 step: 15575 val loss: 1.297013759613037\n",
      "epoch: 31 step: 15600 val loss: 1.3029835224151611\n",
      "epoch: 31 step: 15625 val loss: 1.3036478757858276\n",
      "epoch: 31 step: 15650 val loss: 1.3017594814300537\n",
      "epoch: 31 step: 15675 val loss: 1.3040515184402466\n",
      "epoch: 31 step: 15700 val loss: 1.303173303604126\n",
      "epoch: 31 step: 15725 val loss: 1.303054690361023\n",
      "epoch: 32 step: 15750 val loss: 1.304242730140686\n",
      "epoch: 32 step: 15775 val loss: 1.306136965751648\n",
      "epoch: 32 step: 15800 val loss: 1.3047105073928833\n",
      "epoch: 32 step: 15825 val loss: 1.2974597215652466\n",
      "epoch: 32 step: 15850 val loss: 1.2963556051254272\n",
      "epoch: 32 step: 15875 val loss: 1.2999060153961182\n",
      "epoch: 32 step: 15900 val loss: 1.299004077911377\n",
      "epoch: 32 step: 15925 val loss: 1.2977802753448486\n",
      "epoch: 32 step: 15950 val loss: 1.2966009378433228\n",
      "epoch: 32 step: 15975 val loss: 1.2952601909637451\n",
      "epoch: 32 step: 16000 val loss: 1.2941786050796509\n",
      "epoch: 32 step: 16025 val loss: 1.2969024181365967\n",
      "epoch: 32 step: 16050 val loss: 1.2941420078277588\n",
      "epoch: 32 step: 16075 val loss: 1.2957773208618164\n",
      "epoch: 32 step: 16100 val loss: 1.3018282651901245\n",
      "epoch: 32 step: 16125 val loss: 1.3015005588531494\n",
      "epoch: 32 step: 16150 val loss: 1.3054914474487305\n",
      "epoch: 32 step: 16175 val loss: 1.3054262399673462\n",
      "epoch: 32 step: 16200 val loss: 1.3049448728561401\n",
      "epoch: 33 step: 16225 val loss: 1.3036377429962158\n",
      "epoch: 33 step: 16250 val loss: 1.3024609088897705\n",
      "epoch: 33 step: 16275 val loss: 1.3041844367980957\n",
      "epoch: 33 step: 16300 val loss: 1.3029494285583496\n",
      "epoch: 33 step: 16325 val loss: 1.3047423362731934\n",
      "epoch: 33 step: 16350 val loss: 1.296923041343689\n",
      "epoch: 33 step: 16375 val loss: 1.2998759746551514\n",
      "epoch: 33 step: 16400 val loss: 1.303239107131958\n",
      "epoch: 33 step: 16425 val loss: 1.3011795282363892\n",
      "epoch: 33 step: 16450 val loss: 1.2957651615142822\n",
      "epoch: 33 step: 16475 val loss: 1.288815975189209\n",
      "epoch: 33 step: 16500 val loss: 1.291123867034912\n",
      "epoch: 33 step: 16525 val loss: 1.2942166328430176\n",
      "epoch: 33 step: 16550 val loss: 1.289131760597229\n",
      "epoch: 33 step: 16575 val loss: 1.2983211278915405\n",
      "epoch: 33 step: 16600 val loss: 1.29787015914917\n",
      "epoch: 33 step: 16625 val loss: 1.29781174659729\n",
      "epoch: 33 step: 16650 val loss: 1.3036853075027466\n",
      "epoch: 33 step: 16675 val loss: 1.2996423244476318\n",
      "epoch: 33 step: 16700 val loss: 1.2962807416915894\n",
      "epoch: 34 step: 16725 val loss: 1.2969036102294922\n",
      "epoch: 34 step: 16750 val loss: 1.3015943765640259\n",
      "epoch: 34 step: 16775 val loss: 1.3064684867858887\n",
      "epoch: 34 step: 16800 val loss: 1.2968180179595947\n",
      "epoch: 34 step: 16825 val loss: 1.2982357740402222\n",
      "epoch: 34 step: 16850 val loss: 1.2982919216156006\n",
      "epoch: 34 step: 16875 val loss: 1.2988462448120117\n",
      "epoch: 34 step: 16900 val loss: 1.2966828346252441\n",
      "epoch: 34 step: 16925 val loss: 1.2896732091903687\n",
      "epoch: 34 step: 16950 val loss: 1.2947423458099365\n",
      "epoch: 34 step: 16975 val loss: 1.2935149669647217\n",
      "epoch: 34 step: 17000 val loss: 1.293082356452942\n",
      "epoch: 34 step: 17025 val loss: 1.292838215827942\n",
      "epoch: 34 step: 17050 val loss: 1.2951141595840454\n",
      "epoch: 34 step: 17075 val loss: 1.2994798421859741\n",
      "epoch: 34 step: 17100 val loss: 1.3010976314544678\n",
      "epoch: 34 step: 17125 val loss: 1.299835205078125\n",
      "epoch: 34 step: 17150 val loss: 1.3001242876052856\n",
      "epoch: 34 step: 17175 val loss: 1.30369234085083\n",
      "epoch: 34 step: 17200 val loss: 1.300408959388733\n",
      "epoch: 35 step: 17225 val loss: 1.3010023832321167\n",
      "epoch: 35 step: 17250 val loss: 1.3049670457839966\n",
      "epoch: 35 step: 17275 val loss: 1.3005177974700928\n",
      "epoch: 35 step: 17300 val loss: 1.299038052558899\n",
      "epoch: 35 step: 17325 val loss: 1.296850562095642\n",
      "epoch: 35 step: 17350 val loss: 1.3001303672790527\n",
      "epoch: 35 step: 17375 val loss: 1.2971818447113037\n",
      "epoch: 35 step: 17400 val loss: 1.2993711233139038\n",
      "epoch: 35 step: 17425 val loss: 1.299149751663208\n",
      "epoch: 35 step: 17450 val loss: 1.294893741607666\n",
      "epoch: 35 step: 17475 val loss: 1.2931718826293945\n",
      "epoch: 35 step: 17500 val loss: 1.2973061800003052\n",
      "epoch: 35 step: 17525 val loss: 1.2929151058197021\n",
      "epoch: 35 step: 17550 val loss: 1.300388216972351\n",
      "epoch: 35 step: 17575 val loss: 1.2977021932601929\n",
      "epoch: 35 step: 17600 val loss: 1.297791838645935\n",
      "epoch: 35 step: 17625 val loss: 1.2967863082885742\n",
      "epoch: 35 step: 17650 val loss: 1.301081657409668\n",
      "epoch: 35 step: 17675 val loss: 1.298943042755127\n",
      "epoch: 36 step: 17700 val loss: 1.3005753755569458\n",
      "epoch: 36 step: 17725 val loss: 1.3035765886306763\n",
      "epoch: 36 step: 17750 val loss: 1.3022239208221436\n",
      "epoch: 36 step: 17775 val loss: 1.3036919832229614\n",
      "epoch: 36 step: 17800 val loss: 1.2969756126403809\n",
      "epoch: 36 step: 17825 val loss: 1.292682409286499\n",
      "epoch: 36 step: 17850 val loss: 1.2950819730758667\n",
      "epoch: 36 step: 17875 val loss: 1.2928686141967773\n",
      "epoch: 36 step: 17900 val loss: 1.2929927110671997\n",
      "epoch: 36 step: 17925 val loss: 1.2918708324432373\n",
      "epoch: 36 step: 17950 val loss: 1.290053129196167\n",
      "epoch: 36 step: 17975 val loss: 1.2953811883926392\n",
      "epoch: 36 step: 18000 val loss: 1.2981301546096802\n",
      "epoch: 36 step: 18025 val loss: 1.2986266613006592\n",
      "epoch: 36 step: 18050 val loss: 1.3023852109909058\n",
      "epoch: 36 step: 18075 val loss: 1.3043079376220703\n",
      "epoch: 36 step: 18100 val loss: 1.3045716285705566\n",
      "epoch: 36 step: 18125 val loss: 1.3054808378219604\n",
      "epoch: 36 step: 18150 val loss: 1.306951642036438\n",
      "epoch: 36 step: 18175 val loss: 1.305283546447754\n",
      "epoch: 37 step: 18200 val loss: 1.3033156394958496\n",
      "epoch: 37 step: 18225 val loss: 1.302659273147583\n",
      "epoch: 37 step: 18250 val loss: 1.304480791091919\n",
      "epoch: 37 step: 18275 val loss: 1.2960747480392456\n",
      "epoch: 37 step: 18300 val loss: 1.3026793003082275\n",
      "epoch: 37 step: 18325 val loss: 1.2970337867736816\n",
      "epoch: 37 step: 18350 val loss: 1.2950040102005005\n",
      "epoch: 37 step: 18375 val loss: 1.2942084074020386\n",
      "epoch: 37 step: 18400 val loss: 1.2911643981933594\n",
      "epoch: 37 step: 18425 val loss: 1.290450096130371\n",
      "epoch: 37 step: 18450 val loss: 1.2919983863830566\n",
      "epoch: 37 step: 18475 val loss: 1.2886607646942139\n",
      "epoch: 37 step: 18500 val loss: 1.288124442100525\n",
      "epoch: 37 step: 18525 val loss: 1.2916899919509888\n",
      "epoch: 37 step: 18550 val loss: 1.2942665815353394\n",
      "epoch: 37 step: 18575 val loss: 1.298457384109497\n",
      "epoch: 37 step: 18600 val loss: 1.2984991073608398\n",
      "epoch: 37 step: 18625 val loss: 1.298417329788208\n",
      "epoch: 37 step: 18650 val loss: 1.2969292402267456\n",
      "epoch: 38 step: 18675 val loss: 1.2985996007919312\n",
      "epoch: 38 step: 18700 val loss: 1.3038363456726074\n",
      "epoch: 38 step: 18725 val loss: 1.3035799264907837\n",
      "epoch: 38 step: 18750 val loss: 1.303740382194519\n",
      "epoch: 38 step: 18775 val loss: 1.3039034605026245\n",
      "epoch: 38 step: 18800 val loss: 1.2941218614578247\n",
      "epoch: 38 step: 18825 val loss: 1.2981599569320679\n",
      "epoch: 38 step: 18850 val loss: 1.2964556217193604\n",
      "epoch: 38 step: 18875 val loss: 1.2944128513336182\n",
      "epoch: 38 step: 18900 val loss: 1.2924227714538574\n",
      "epoch: 38 step: 18925 val loss: 1.2884637117385864\n",
      "epoch: 38 step: 18950 val loss: 1.2919228076934814\n",
      "epoch: 38 step: 18975 val loss: 1.2962766885757446\n",
      "epoch: 38 step: 19000 val loss: 1.2935655117034912\n",
      "epoch: 38 step: 19025 val loss: 1.3000580072402954\n",
      "epoch: 38 step: 19050 val loss: 1.2995692491531372\n",
      "epoch: 38 step: 19075 val loss: 1.2971333265304565\n",
      "epoch: 38 step: 19100 val loss: 1.2998937368392944\n",
      "epoch: 38 step: 19125 val loss: 1.3001694679260254\n",
      "epoch: 38 step: 19150 val loss: 1.3020581007003784\n",
      "epoch: 39 step: 19175 val loss: 1.3001976013183594\n",
      "epoch: 39 step: 19200 val loss: 1.2983839511871338\n",
      "epoch: 39 step: 19225 val loss: 1.301893711090088\n",
      "epoch: 39 step: 19250 val loss: 1.294917106628418\n",
      "epoch: 39 step: 19275 val loss: 1.2995667457580566\n",
      "epoch: 39 step: 19300 val loss: 1.2974647283554077\n",
      "epoch: 39 step: 19325 val loss: 1.3051533699035645\n",
      "epoch: 39 step: 19350 val loss: 1.3031132221221924\n",
      "epoch: 39 step: 19375 val loss: 1.2967602014541626\n",
      "epoch: 39 step: 19400 val loss: 1.297087550163269\n",
      "epoch: 39 step: 19425 val loss: 1.293628454208374\n",
      "epoch: 39 step: 19450 val loss: 1.295244812965393\n",
      "epoch: 39 step: 19475 val loss: 1.296202540397644\n",
      "epoch: 39 step: 19500 val loss: 1.3014174699783325\n",
      "epoch: 39 step: 19525 val loss: 1.3021551370620728\n",
      "epoch: 39 step: 19550 val loss: 1.3051118850708008\n",
      "epoch: 39 step: 19575 val loss: 1.3061480522155762\n",
      "epoch: 39 step: 19600 val loss: 1.3063949346542358\n",
      "epoch: 39 step: 19625 val loss: 1.3045721054077148\n",
      "epoch: 39 step: 19650 val loss: 1.306306004524231\n",
      "epoch: 40 step: 19675 val loss: 1.3013921976089478\n",
      "epoch: 40 step: 19700 val loss: 1.3080042600631714\n",
      "epoch: 40 step: 19725 val loss: 1.3079947233200073\n",
      "epoch: 40 step: 19750 val loss: 1.3027558326721191\n",
      "epoch: 40 step: 19775 val loss: 1.29826819896698\n",
      "epoch: 40 step: 19800 val loss: 1.3044439554214478\n",
      "epoch: 40 step: 19825 val loss: 1.2992637157440186\n",
      "epoch: 40 step: 19850 val loss: 1.2992368936538696\n",
      "epoch: 40 step: 19875 val loss: 1.29977285861969\n",
      "epoch: 40 step: 19900 val loss: 1.2930593490600586\n",
      "epoch: 40 step: 19925 val loss: 1.297050952911377\n",
      "epoch: 40 step: 19950 val loss: 1.3024418354034424\n",
      "epoch: 40 step: 19975 val loss: 1.293312907218933\n",
      "epoch: 40 step: 20000 val loss: 1.3010815382003784\n",
      "epoch: 40 step: 20025 val loss: 1.299805998802185\n",
      "epoch: 40 step: 20050 val loss: 1.2977042198181152\n",
      "epoch: 40 step: 20075 val loss: 1.2994763851165771\n",
      "epoch: 40 step: 20100 val loss: 1.3010175228118896\n",
      "epoch: 40 step: 20125 val loss: 1.3004024028778076\n",
      "epoch: 41 step: 20150 val loss: 1.3034429550170898\n",
      "epoch: 41 step: 20175 val loss: 1.300318717956543\n",
      "epoch: 41 step: 20200 val loss: 1.3030672073364258\n",
      "epoch: 41 step: 20225 val loss: 1.2961385250091553\n",
      "epoch: 41 step: 20250 val loss: 1.295674443244934\n",
      "epoch: 41 step: 20275 val loss: 1.2927987575531006\n",
      "epoch: 41 step: 20300 val loss: 1.2969506978988647\n",
      "epoch: 41 step: 20325 val loss: 1.2958123683929443\n",
      "epoch: 41 step: 20350 val loss: 1.2933013439178467\n",
      "epoch: 41 step: 20375 val loss: 1.2922643423080444\n",
      "epoch: 41 step: 20400 val loss: 1.291164517402649\n",
      "epoch: 41 step: 20425 val loss: 1.2922290563583374\n",
      "epoch: 41 step: 20450 val loss: 1.2951717376708984\n",
      "epoch: 41 step: 20475 val loss: 1.2970699071884155\n",
      "epoch: 41 step: 20500 val loss: 1.2984070777893066\n",
      "epoch: 41 step: 20525 val loss: 1.3008393049240112\n",
      "epoch: 41 step: 20550 val loss: 1.3015066385269165\n",
      "epoch: 41 step: 20575 val loss: 1.3042951822280884\n",
      "epoch: 41 step: 20600 val loss: 1.2995541095733643\n",
      "epoch: 41 step: 20625 val loss: 1.3039714097976685\n",
      "epoch: 42 step: 20650 val loss: 1.3023470640182495\n",
      "epoch: 42 step: 20675 val loss: 1.3072144985198975\n",
      "epoch: 42 step: 20700 val loss: 1.3078131675720215\n",
      "epoch: 42 step: 20725 val loss: 1.3018585443496704\n",
      "epoch: 42 step: 20750 val loss: 1.3025341033935547\n",
      "epoch: 42 step: 20775 val loss: 1.2990477085113525\n",
      "epoch: 42 step: 20800 val loss: 1.2956496477127075\n",
      "epoch: 42 step: 20825 val loss: 1.301176905632019\n",
      "epoch: 42 step: 20850 val loss: 1.2981551885604858\n",
      "epoch: 42 step: 20875 val loss: 1.2970575094223022\n",
      "epoch: 42 step: 20900 val loss: 1.2959740161895752\n",
      "epoch: 42 step: 20925 val loss: 1.2999833822250366\n",
      "epoch: 42 step: 20950 val loss: 1.2942456007003784\n",
      "epoch: 42 step: 20975 val loss: 1.3000112771987915\n",
      "epoch: 42 step: 21000 val loss: 1.301958680152893\n",
      "epoch: 42 step: 21025 val loss: 1.3054876327514648\n",
      "epoch: 42 step: 21050 val loss: 1.302957534790039\n",
      "epoch: 42 step: 21075 val loss: 1.2999582290649414\n",
      "epoch: 42 step: 21100 val loss: 1.3020392656326294\n",
      "epoch: 43 step: 21125 val loss: 1.3061542510986328\n",
      "epoch: 43 step: 21150 val loss: 1.2927844524383545\n",
      "epoch: 43 step: 21175 val loss: 1.2921104431152344\n",
      "epoch: 43 step: 21200 val loss: 1.2875012159347534\n",
      "epoch: 43 step: 21225 val loss: 1.2899856567382812\n",
      "epoch: 43 step: 21250 val loss: 1.2831175327301025\n",
      "epoch: 43 step: 21275 val loss: 1.282928466796875\n",
      "epoch: 43 step: 21300 val loss: 1.2862437963485718\n",
      "epoch: 43 step: 21325 val loss: 1.2859801054000854\n",
      "epoch: 43 step: 21350 val loss: 1.2872765064239502\n",
      "epoch: 43 step: 21375 val loss: 1.2886104583740234\n",
      "epoch: 43 step: 21400 val loss: 1.2895163297653198\n",
      "epoch: 43 step: 21425 val loss: 1.2883474826812744\n",
      "epoch: 43 step: 21450 val loss: 1.2859891653060913\n",
      "epoch: 43 step: 21475 val loss: 1.2926135063171387\n",
      "epoch: 43 step: 21500 val loss: 1.289552092552185\n",
      "epoch: 43 step: 21525 val loss: 1.2915301322937012\n",
      "epoch: 43 step: 21550 val loss: 1.295497179031372\n",
      "epoch: 43 step: 21575 val loss: 1.2954169511795044\n",
      "epoch: 43 step: 21600 val loss: 1.2983827590942383\n",
      "epoch: 44 step: 21625 val loss: 1.2977591753005981\n",
      "epoch: 44 step: 21650 val loss: 1.2978112697601318\n",
      "epoch: 44 step: 21675 val loss: 1.3013825416564941\n",
      "epoch: 44 step: 21700 val loss: 1.29720139503479\n",
      "epoch: 44 step: 21725 val loss: 1.2993074655532837\n",
      "epoch: 44 step: 21750 val loss: 1.2999001741409302\n",
      "epoch: 44 step: 21775 val loss: 1.3040355443954468\n",
      "epoch: 44 step: 21800 val loss: 1.3039780855178833\n",
      "epoch: 44 step: 21825 val loss: 1.2962077856063843\n",
      "epoch: 44 step: 21850 val loss: 1.2957814931869507\n",
      "epoch: 44 step: 21875 val loss: 1.2959645986557007\n",
      "epoch: 44 step: 21900 val loss: 1.295157790184021\n",
      "epoch: 44 step: 21925 val loss: 1.2984269857406616\n",
      "epoch: 44 step: 21950 val loss: 1.3011384010314941\n",
      "epoch: 44 step: 21975 val loss: 1.3041634559631348\n",
      "epoch: 44 step: 22000 val loss: 1.306626319885254\n",
      "epoch: 44 step: 22025 val loss: 1.3003023862838745\n",
      "epoch: 44 step: 22050 val loss: 1.3011246919631958\n",
      "epoch: 44 step: 22075 val loss: 1.3042523860931396\n",
      "epoch: 44 step: 22100 val loss: 1.3044252395629883\n",
      "epoch: 45 step: 22125 val loss: 1.29619562625885\n",
      "epoch: 45 step: 22150 val loss: 1.2996631860733032\n",
      "epoch: 45 step: 22175 val loss: 1.299989938735962\n",
      "epoch: 45 step: 22200 val loss: 1.2983784675598145\n",
      "epoch: 45 step: 22225 val loss: 1.294095754623413\n",
      "epoch: 45 step: 22250 val loss: 1.2969722747802734\n",
      "epoch: 45 step: 22275 val loss: 1.291262149810791\n",
      "epoch: 45 step: 22300 val loss: 1.2956386804580688\n",
      "epoch: 45 step: 22325 val loss: 1.2962098121643066\n",
      "epoch: 45 step: 22350 val loss: 1.2948628664016724\n",
      "epoch: 45 step: 22375 val loss: 1.2990036010742188\n",
      "epoch: 45 step: 22400 val loss: 1.3004236221313477\n",
      "epoch: 45 step: 22425 val loss: 1.2938898801803589\n",
      "epoch: 45 step: 22450 val loss: 1.296762228012085\n",
      "epoch: 45 step: 22475 val loss: 1.2958766222000122\n",
      "epoch: 45 step: 22500 val loss: 1.296256422996521\n",
      "epoch: 45 step: 22525 val loss: 1.29970383644104\n",
      "epoch: 45 step: 22550 val loss: 1.2982392311096191\n",
      "epoch: 45 step: 22575 val loss: 1.300036072731018\n",
      "epoch: 46 step: 22600 val loss: 1.2945233583450317\n",
      "epoch: 46 step: 22625 val loss: 1.2998683452606201\n",
      "epoch: 46 step: 22650 val loss: 1.2988901138305664\n",
      "epoch: 46 step: 22675 val loss: 1.29524564743042\n",
      "epoch: 46 step: 22700 val loss: 1.2983360290527344\n",
      "epoch: 46 step: 22725 val loss: 1.2978627681732178\n",
      "epoch: 46 step: 22750 val loss: 1.2976741790771484\n",
      "epoch: 46 step: 22775 val loss: 1.302198052406311\n",
      "epoch: 46 step: 22800 val loss: 1.2958828210830688\n",
      "epoch: 46 step: 22825 val loss: 1.297073483467102\n",
      "epoch: 46 step: 22850 val loss: 1.2958356142044067\n",
      "epoch: 46 step: 22875 val loss: 1.2942136526107788\n",
      "epoch: 46 step: 22900 val loss: 1.2985906600952148\n",
      "epoch: 46 step: 22925 val loss: 1.297788381576538\n",
      "epoch: 46 step: 22950 val loss: 1.3021039962768555\n",
      "epoch: 46 step: 22975 val loss: 1.306810975074768\n",
      "epoch: 46 step: 23000 val loss: 1.3068264722824097\n",
      "epoch: 46 step: 23025 val loss: 1.305665373802185\n",
      "epoch: 46 step: 23050 val loss: 1.3035205602645874\n",
      "epoch: 46 step: 23075 val loss: 1.3083884716033936\n",
      "epoch: 47 step: 23100 val loss: 1.3069519996643066\n",
      "epoch: 47 step: 23125 val loss: 1.308700680732727\n",
      "epoch: 47 step: 23150 val loss: 1.3034567832946777\n",
      "epoch: 47 step: 23175 val loss: 1.3001924753189087\n",
      "epoch: 47 step: 23200 val loss: 1.3084062337875366\n",
      "epoch: 47 step: 23225 val loss: 1.2981770038604736\n",
      "epoch: 47 step: 23250 val loss: 1.294768214225769\n",
      "epoch: 47 step: 23275 val loss: 1.2967058420181274\n",
      "epoch: 47 step: 23300 val loss: 1.3043264150619507\n",
      "epoch: 47 step: 23325 val loss: 1.3008475303649902\n",
      "epoch: 47 step: 23350 val loss: 1.3035656213760376\n",
      "epoch: 47 step: 23375 val loss: 1.3051215410232544\n",
      "epoch: 47 step: 23400 val loss: 1.3021472692489624\n",
      "epoch: 47 step: 23425 val loss: 1.302627682685852\n",
      "epoch: 47 step: 23450 val loss: 1.3040530681610107\n",
      "epoch: 47 step: 23475 val loss: 1.3086307048797607\n",
      "epoch: 47 step: 23500 val loss: 1.3054254055023193\n",
      "epoch: 47 step: 23525 val loss: 1.3047312498092651\n",
      "epoch: 47 step: 23550 val loss: 1.298569679260254\n",
      "epoch: 48 step: 23575 val loss: 1.3015435934066772\n",
      "epoch: 48 step: 23600 val loss: 1.3016818761825562\n",
      "epoch: 48 step: 23625 val loss: 1.3006870746612549\n",
      "epoch: 48 step: 23650 val loss: 1.2995857000350952\n",
      "epoch: 48 step: 23675 val loss: 1.3063572645187378\n",
      "epoch: 48 step: 23700 val loss: 1.2930806875228882\n",
      "epoch: 48 step: 23725 val loss: 1.3003827333450317\n",
      "epoch: 48 step: 23750 val loss: 1.3028671741485596\n",
      "epoch: 48 step: 23775 val loss: 1.3075352907180786\n",
      "epoch: 48 step: 23800 val loss: 1.306320071220398\n",
      "epoch: 48 step: 23825 val loss: 1.30238676071167\n",
      "epoch: 48 step: 23850 val loss: 1.3010945320129395\n",
      "epoch: 48 step: 23875 val loss: 1.3043144941329956\n",
      "epoch: 48 step: 23900 val loss: 1.301497220993042\n",
      "epoch: 48 step: 23925 val loss: 1.3082821369171143\n",
      "epoch: 48 step: 23950 val loss: 1.3060377836227417\n",
      "epoch: 48 step: 23975 val loss: 1.3071576356887817\n",
      "epoch: 48 step: 24000 val loss: 1.3090652227401733\n",
      "epoch: 48 step: 24025 val loss: 1.3079675436019897\n",
      "epoch: 48 step: 24050 val loss: 1.3037160634994507\n",
      "epoch: 49 step: 24075 val loss: 1.3036129474639893\n",
      "epoch: 49 step: 24100 val loss: 1.308060884475708\n",
      "epoch: 49 step: 24125 val loss: 1.3077709674835205\n",
      "epoch: 49 step: 24150 val loss: 1.3085037469863892\n",
      "epoch: 49 step: 24175 val loss: 1.305511236190796\n",
      "epoch: 49 step: 24200 val loss: 1.3046520948410034\n",
      "epoch: 49 step: 24225 val loss: 1.307280421257019\n",
      "epoch: 49 step: 24250 val loss: 1.3010735511779785\n",
      "epoch: 49 step: 24275 val loss: 1.2976421117782593\n",
      "epoch: 49 step: 24300 val loss: 1.3112746477127075\n",
      "epoch: 49 step: 24325 val loss: 1.3001290559768677\n",
      "epoch: 49 step: 24350 val loss: 1.2977548837661743\n",
      "epoch: 49 step: 24375 val loss: 1.2966762781143188\n",
      "epoch: 49 step: 24400 val loss: 1.2967092990875244\n",
      "epoch: 49 step: 24425 val loss: 1.2997123003005981\n",
      "epoch: 49 step: 24450 val loss: 1.3058652877807617\n",
      "epoch: 49 step: 24475 val loss: 1.3040037155151367\n",
      "epoch: 49 step: 24500 val loss: 1.3054418563842773\n",
      "epoch: 49 step: 24525 val loss: 1.3057935237884521\n",
      "epoch: 49 step: 24550 val loss: 1.305640697479248\n",
      "epoch: 50 step: 24575 val loss: 1.3093771934509277\n",
      "epoch: 50 step: 24600 val loss: 1.3080726861953735\n",
      "epoch: 50 step: 24625 val loss: 1.3046587705612183\n",
      "epoch: 50 step: 24650 val loss: 1.3049900531768799\n",
      "epoch: 50 step: 24675 val loss: 1.305849552154541\n",
      "epoch: 50 step: 24700 val loss: 1.3014836311340332\n",
      "epoch: 50 step: 24725 val loss: 1.2975455522537231\n",
      "epoch: 50 step: 24750 val loss: 1.3044646978378296\n",
      "epoch: 50 step: 24775 val loss: 1.3023241758346558\n",
      "epoch: 50 step: 24800 val loss: 1.3002501726150513\n",
      "epoch: 50 step: 24825 val loss: 1.3013283014297485\n",
      "epoch: 50 step: 24850 val loss: 1.3049758672714233\n",
      "epoch: 50 step: 24875 val loss: 1.2990829944610596\n",
      "epoch: 50 step: 24900 val loss: 1.3047786951065063\n",
      "epoch: 50 step: 24925 val loss: 1.3028507232666016\n",
      "epoch: 50 step: 24950 val loss: 1.3030672073364258\n",
      "epoch: 50 step: 24975 val loss: 1.3056076765060425\n",
      "epoch: 50 step: 25000 val loss: 1.3003950119018555\n",
      "epoch: 50 step: 25025 val loss: 1.3054414987564087\n",
      "epoch: 51 step: 25050 val loss: 1.306091547012329\n",
      "epoch: 51 step: 25075 val loss: 1.308945894241333\n",
      "epoch: 51 step: 25100 val loss: 1.3095805644989014\n",
      "epoch: 51 step: 25125 val loss: 1.305914044380188\n",
      "epoch: 51 step: 25150 val loss: 1.3094063997268677\n",
      "epoch: 51 step: 25175 val loss: 1.3037524223327637\n",
      "epoch: 51 step: 25200 val loss: 1.301910400390625\n",
      "epoch: 51 step: 25225 val loss: 1.3089840412139893\n",
      "epoch: 51 step: 25250 val loss: 1.3051271438598633\n",
      "epoch: 51 step: 25275 val loss: 1.3043464422225952\n",
      "epoch: 51 step: 25300 val loss: 1.2990590333938599\n",
      "epoch: 51 step: 25325 val loss: 1.2965220212936401\n",
      "epoch: 51 step: 25350 val loss: 1.3032686710357666\n",
      "epoch: 51 step: 25375 val loss: 1.30375075340271\n",
      "epoch: 51 step: 25400 val loss: 1.3058727979660034\n",
      "epoch: 51 step: 25425 val loss: 1.309695839881897\n",
      "epoch: 51 step: 25450 val loss: 1.3069509267807007\n",
      "epoch: 51 step: 25475 val loss: 1.3086694478988647\n",
      "epoch: 51 step: 25500 val loss: 1.307148814201355\n",
      "epoch: 51 step: 25525 val loss: 1.3033225536346436\n",
      "epoch: 52 step: 25550 val loss: 1.305667757987976\n",
      "epoch: 52 step: 25575 val loss: 1.3048492670059204\n",
      "epoch: 52 step: 25600 val loss: 1.3049200773239136\n",
      "epoch: 52 step: 25625 val loss: 1.3037256002426147\n",
      "epoch: 52 step: 25650 val loss: 1.3057949542999268\n",
      "epoch: 52 step: 25675 val loss: 1.300758957862854\n",
      "epoch: 52 step: 25700 val loss: 1.3001030683517456\n",
      "epoch: 52 step: 25725 val loss: 1.298038363456726\n",
      "epoch: 52 step: 25750 val loss: 1.2967524528503418\n",
      "epoch: 52 step: 25775 val loss: 1.2968961000442505\n",
      "epoch: 52 step: 25800 val loss: 1.2931153774261475\n",
      "epoch: 52 step: 25825 val loss: 1.295958399772644\n",
      "epoch: 52 step: 25850 val loss: 1.2977923154830933\n",
      "epoch: 52 step: 25875 val loss: 1.2994439601898193\n",
      "epoch: 52 step: 25900 val loss: 1.300641417503357\n",
      "epoch: 52 step: 25925 val loss: 1.3040074110031128\n",
      "epoch: 52 step: 25950 val loss: 1.3028395175933838\n",
      "epoch: 52 step: 25975 val loss: 1.3033527135849\n",
      "epoch: 52 step: 26000 val loss: 1.306639552116394\n",
      "epoch: 53 step: 26025 val loss: 1.3048055171966553\n",
      "epoch: 53 step: 26050 val loss: 1.3089739084243774\n",
      "epoch: 53 step: 26075 val loss: 1.3017617464065552\n",
      "epoch: 53 step: 26100 val loss: 1.2990189790725708\n",
      "epoch: 53 step: 26125 val loss: 1.3018386363983154\n",
      "epoch: 53 step: 26150 val loss: 1.2952985763549805\n",
      "epoch: 53 step: 26175 val loss: 1.3049246072769165\n",
      "epoch: 53 step: 26200 val loss: 1.302720069885254\n",
      "epoch: 53 step: 26225 val loss: 1.3053345680236816\n",
      "epoch: 53 step: 26250 val loss: 1.3042535781860352\n",
      "epoch: 53 step: 26275 val loss: 1.301058053970337\n",
      "epoch: 53 step: 26300 val loss: 1.3017915487289429\n",
      "epoch: 53 step: 26325 val loss: 1.3011504411697388\n",
      "epoch: 53 step: 26350 val loss: 1.3020122051239014\n",
      "epoch: 53 step: 26375 val loss: 1.306776762008667\n",
      "epoch: 53 step: 26400 val loss: 1.3036906719207764\n",
      "epoch: 53 step: 26425 val loss: 1.3062158823013306\n",
      "epoch: 53 step: 26450 val loss: 1.3101199865341187\n",
      "epoch: 53 step: 26475 val loss: 1.3104320764541626\n",
      "epoch: 53 step: 26500 val loss: 1.3133352994918823\n",
      "epoch: 54 step: 26525 val loss: 1.3087708950042725\n",
      "epoch: 54 step: 26550 val loss: 1.3105298280715942\n",
      "epoch: 54 step: 26575 val loss: 1.313529372215271\n",
      "epoch: 54 step: 26600 val loss: 1.3062219619750977\n",
      "epoch: 54 step: 26625 val loss: 1.3047763109207153\n",
      "epoch: 54 step: 26650 val loss: 1.303201675415039\n",
      "epoch: 54 step: 26675 val loss: 1.3075562715530396\n",
      "epoch: 54 step: 26700 val loss: 1.3054144382476807\n",
      "epoch: 54 step: 26725 val loss: 1.3032461404800415\n",
      "epoch: 54 step: 26750 val loss: 1.30158269405365\n",
      "epoch: 54 step: 26775 val loss: 1.2980878353118896\n",
      "epoch: 54 step: 26800 val loss: 1.2987843751907349\n",
      "epoch: 54 step: 26825 val loss: 1.3029017448425293\n",
      "epoch: 54 step: 26850 val loss: 1.3061975240707397\n",
      "epoch: 54 step: 26875 val loss: 1.307621717453003\n",
      "epoch: 54 step: 26900 val loss: 1.3075553178787231\n",
      "epoch: 54 step: 26925 val loss: 1.3097312450408936\n",
      "epoch: 54 step: 26950 val loss: 1.30912184715271\n",
      "epoch: 54 step: 26975 val loss: 1.3047617673873901\n",
      "epoch: 54 step: 27000 val loss: 1.3068853616714478\n",
      "epoch: 55 step: 27025 val loss: 1.3050537109375\n",
      "epoch: 55 step: 27050 val loss: 1.3069385290145874\n",
      "epoch: 55 step: 27075 val loss: 1.3063665628433228\n",
      "epoch: 55 step: 27100 val loss: 1.3013544082641602\n",
      "epoch: 55 step: 27125 val loss: 1.3005492687225342\n",
      "epoch: 55 step: 27150 val loss: 1.298714280128479\n",
      "epoch: 55 step: 27175 val loss: 1.2961134910583496\n",
      "epoch: 55 step: 27200 val loss: 1.2958638668060303\n",
      "epoch: 55 step: 27225 val loss: 1.2980000972747803\n",
      "epoch: 55 step: 27250 val loss: 1.2921782732009888\n",
      "epoch: 55 step: 27275 val loss: 1.2970423698425293\n",
      "epoch: 55 step: 27300 val loss: 1.2970983982086182\n",
      "epoch: 55 step: 27325 val loss: 1.2942399978637695\n",
      "epoch: 55 step: 27350 val loss: 1.302242398262024\n",
      "epoch: 55 step: 27375 val loss: 1.298331379890442\n",
      "epoch: 55 step: 27400 val loss: 1.2998088598251343\n",
      "epoch: 55 step: 27425 val loss: 1.297698974609375\n",
      "epoch: 55 step: 27450 val loss: 1.3005033731460571\n",
      "epoch: 55 step: 27475 val loss: 1.2999094724655151\n",
      "epoch: 56 step: 27500 val loss: 1.3034385442733765\n",
      "epoch: 56 step: 27525 val loss: 1.306861400604248\n",
      "epoch: 56 step: 27550 val loss: 1.3048579692840576\n",
      "epoch: 56 step: 27575 val loss: 1.302365779876709\n",
      "epoch: 56 step: 27600 val loss: 1.2991584539413452\n",
      "epoch: 56 step: 27625 val loss: 1.2977023124694824\n",
      "epoch: 56 step: 27650 val loss: 1.3010936975479126\n",
      "epoch: 56 step: 27675 val loss: 1.3057767152786255\n",
      "epoch: 56 step: 27700 val loss: 1.3018250465393066\n",
      "epoch: 56 step: 27725 val loss: 1.3023507595062256\n",
      "epoch: 56 step: 27750 val loss: 1.2960965633392334\n",
      "epoch: 56 step: 27775 val loss: 1.345350742340088\n",
      "epoch: 56 step: 27800 val loss: 1.2957669496536255\n",
      "epoch: 56 step: 27825 val loss: 1.2825802564620972\n",
      "epoch: 56 step: 27850 val loss: 1.282392978668213\n",
      "epoch: 56 step: 27875 val loss: 1.2862460613250732\n",
      "epoch: 56 step: 27900 val loss: 1.2913528680801392\n",
      "epoch: 56 step: 27925 val loss: 1.292375087738037\n",
      "epoch: 56 step: 27950 val loss: 1.2927354574203491\n",
      "epoch: 56 step: 27975 val loss: 1.2947686910629272\n",
      "epoch: 57 step: 28000 val loss: 1.2947380542755127\n",
      "epoch: 57 step: 28025 val loss: 1.2941750288009644\n",
      "epoch: 57 step: 28050 val loss: 1.2945176362991333\n",
      "epoch: 57 step: 28075 val loss: 1.2911378145217896\n",
      "epoch: 57 step: 28100 val loss: 1.2971192598342896\n",
      "epoch: 57 step: 28125 val loss: 1.2929737567901611\n",
      "epoch: 57 step: 28150 val loss: 1.2931607961654663\n",
      "epoch: 57 step: 28175 val loss: 1.2962496280670166\n",
      "epoch: 57 step: 28200 val loss: 1.2926772832870483\n",
      "epoch: 57 step: 28225 val loss: 1.2911245822906494\n",
      "epoch: 57 step: 28250 val loss: 1.2906668186187744\n",
      "epoch: 57 step: 28275 val loss: 1.2882506847381592\n",
      "epoch: 57 step: 28300 val loss: 1.2942901849746704\n",
      "epoch: 57 step: 28325 val loss: 1.295719861984253\n",
      "epoch: 57 step: 28350 val loss: 1.2990875244140625\n",
      "epoch: 57 step: 28375 val loss: 1.3031691312789917\n",
      "epoch: 57 step: 28400 val loss: 1.3032920360565186\n",
      "epoch: 57 step: 28425 val loss: 1.3052107095718384\n",
      "epoch: 57 step: 28450 val loss: 1.3054990768432617\n",
      "epoch: 58 step: 28475 val loss: 1.3009586334228516\n",
      "epoch: 58 step: 28500 val loss: 1.3030996322631836\n",
      "epoch: 58 step: 28525 val loss: 1.3062692880630493\n",
      "epoch: 58 step: 28550 val loss: 1.3056118488311768\n",
      "epoch: 58 step: 28575 val loss: 1.3063116073608398\n",
      "epoch: 58 step: 28600 val loss: 1.3022106885910034\n",
      "epoch: 58 step: 28625 val loss: 1.3039019107818604\n",
      "epoch: 58 step: 28650 val loss: 1.3009793758392334\n",
      "epoch: 58 step: 28675 val loss: 1.3048808574676514\n",
      "epoch: 58 step: 28700 val loss: 1.3054581880569458\n",
      "epoch: 58 step: 28725 val loss: 1.2977608442306519\n",
      "epoch: 58 step: 28750 val loss: 1.299208641052246\n",
      "epoch: 58 step: 28775 val loss: 1.3022029399871826\n",
      "epoch: 58 step: 28800 val loss: 1.2978945970535278\n",
      "epoch: 58 step: 28825 val loss: 1.3048615455627441\n",
      "epoch: 58 step: 28850 val loss: 1.3029735088348389\n",
      "epoch: 58 step: 28875 val loss: 1.300893783569336\n",
      "epoch: 58 step: 28900 val loss: 1.304086685180664\n",
      "epoch: 58 step: 28925 val loss: 1.3021490573883057\n",
      "epoch: 58 step: 28950 val loss: 1.3046919107437134\n",
      "epoch: 59 step: 28975 val loss: 1.3028082847595215\n",
      "epoch: 59 step: 29000 val loss: 1.3038288354873657\n",
      "epoch: 59 step: 29025 val loss: 1.305426836013794\n",
      "epoch: 59 step: 29050 val loss: 1.3044452667236328\n",
      "epoch: 59 step: 29075 val loss: 1.3004658222198486\n",
      "epoch: 59 step: 29100 val loss: 1.3014038801193237\n",
      "epoch: 59 step: 29125 val loss: 1.3052313327789307\n",
      "epoch: 59 step: 29150 val loss: 1.3009090423583984\n",
      "epoch: 59 step: 29175 val loss: 1.2972074747085571\n",
      "epoch: 59 step: 29200 val loss: 1.301462173461914\n",
      "epoch: 59 step: 29225 val loss: 1.297512412071228\n",
      "epoch: 59 step: 29250 val loss: 1.293497920036316\n",
      "epoch: 59 step: 29275 val loss: 1.2955681085586548\n",
      "epoch: 59 step: 29300 val loss: 1.2963131666183472\n",
      "epoch: 59 step: 29325 val loss: 1.3057609796524048\n",
      "epoch: 59 step: 29350 val loss: 1.3079243898391724\n",
      "epoch: 59 step: 29375 val loss: 1.3073651790618896\n",
      "epoch: 59 step: 29400 val loss: 1.3095695972442627\n",
      "epoch: 59 step: 29425 val loss: 1.310908555984497\n",
      "epoch: 59 step: 29450 val loss: 1.3086061477661133\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "if model.use_gpu:\n",
    "    model.cuda()\n",
    "\n",
    "for i in range(epochs):\n",
    "    hidden = model.hidden_state(batch_size)\n",
    "\n",
    "    for x, y in generate_batches(train_data,batch_size,seq_len):\n",
    "        tracker += 1\n",
    "        x = one_hot_encoder(x, num_char)\n",
    "\n",
    "        inputs =torch.from_numpy(x)\n",
    "        targets = torch.from_numpy(y)\n",
    "\n",
    "        if model.use_gpu:\n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "\n",
    "        hidden = tuple([state.data for state in hidden])\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        lstm_output, hidden = model.forward(inputs, hidden)\n",
    "        loss = criterion(lstm_output, targets.view(batch_size*seq_len).long()) # as long to avoid possible data type error\n",
    "        loss.backward()\n",
    "\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=5) # to avoid gradient exploding\n",
    "        optimizer.step()\n",
    "\n",
    "        if tracker%25 == 0:\n",
    "            val_hidden = model.hidden_state(batch_size)\n",
    "            val_losses = []\n",
    "            model.eval()\n",
    "\n",
    "            for x, y in generate_batches(val_data, batch_size, seq_len):\n",
    "                x = one_hot_encoder(x, num_char)\n",
    "\n",
    "                inputs =torch.from_numpy(x)\n",
    "                targets = torch.from_numpy(y)\n",
    "        \n",
    "                if model.use_gpu:\n",
    "                    inputs = inputs.cuda()\n",
    "                    targets = targets.cuda()\n",
    "\n",
    "                val_hidden = tuple([state.data for state in val_hidden])\n",
    "\n",
    "                lstm_output, val_hidden = model.forward(inputs, val_hidden)\n",
    "                val_loss = criterion(lstm_output, targets.view(batch_size*seq_len).long())\n",
    "\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "            model.train()\n",
    "\n",
    "            print(f'epoch: {i} step: {tracker} val loss: {val_loss.item()}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "94aaabaf-ce1d-4494-87ff-1d38a51f376a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'hidden512_layers3_shakes.net'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7049343f-5e81-4974-b73d-b4ab1421b488",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "be04c395-a65c-4f1b-8bef-b9bf9abe0c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_char(model, char, hidden=None, k=1):\n",
    "    encoded_text = model.encoder[char]\n",
    "    encoded_text = np.array([[encoded_text]])\n",
    "\n",
    "    encoded_text = one_hot_encoder(encoded_text, len(model.all_chars))\n",
    "\n",
    "    inputs = torch.from_numpy(encoded_text)\n",
    "\n",
    "    if model.use_gpu:\n",
    "        inputs = inputs.cuda()\n",
    "\n",
    "    hidden = tuple([state.data for state in hidden])\n",
    "\n",
    "    lstm_out, hidden = model(inputs, hidden)\n",
    "\n",
    "    probs = F.softmax(lstm_out,dim=1).data\n",
    "\n",
    "    if model.use_gpu:\n",
    "        probs = probs.cpu()\n",
    "\n",
    "    probs, idx_positions = probs.topk(k) # k determines how many chars to be considered in this probability\n",
    "    idx_positions = idx_positions.numpy().squeeze() # squeeze to put in a correct shape\n",
    "\n",
    "    probs = probs.numpy().flatten()\n",
    "    probs = probs/probs.sum()\n",
    "\n",
    "    char = np.random.choice(idx_positions, p=probs) # next char\n",
    "\n",
    "    return model.decoder[char], hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "46c2800b-5ee2-405f-82d4-862d65b9e16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, size, seed='The', k=1): # different k may print less probable chars (higher chance of typo)\n",
    "    if model.use_gpu:\n",
    "        model.cuda()\n",
    "    else:\n",
    "        model.cpu()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    output_chars = [c for c in seed]\n",
    "    \n",
    "    hidden = model.hidden_state(1)\n",
    "\n",
    "    for char in seed:\n",
    "        char, hidden = predict_next_char(model, char, hidden, k=k)\n",
    "\n",
    "    output_chars.append(char)\n",
    "\n",
    "    for i in range(size):\n",
    "        char, hidden = predict_next_char(model, output_chars[-1], hidden, k=k) # now generating prediction of prediction\n",
    "        output_chars.append(char)\n",
    "\n",
    "    return ''.join(output_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f56f6392-f7db-40c6-8ff7-0f1d38768368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word of the sun too street of a seal of a strong\n",
      "    dead soul that they will be a sensuble truth in this way.\n",
      "  Ham. Why, what's your honour too?\n",
      "  Bene. I will be a stream of all the stars, and I will never say I am\n",
      "    not a state of man and my son in my heart.\n",
      "  Bene. What say you to me, sir? Why then, I have not a third that you have, and\n",
      "    the matter we have any office of the contrary.\n",
      "  Ham. I am a soldier, some of you.\n",
      "\n",
      "                       Enter Polonius.\n",
      "\n",
      "  Prince. The King himself is, there willst thou be thought\n",
      "    To see my son in the stock of the confines, and\n",
      "    the servant of the sea of the sun, and the months of the truth\n",
      "    as they should say to this world and her stones to the court of the\n",
      "    sea, when his bed and second cheek is three and stops'd to his\n",
      "    son to the sun and his angel.\n",
      "  Ham. That should say I am now to think on the way.\n",
      "  Pedro. Those thoughts to the sea, the storm, and their cause. I will be the\n",
      "    toothagation to be thine, that the star \n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, 1000, seed='The ', k=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede6dc07-7a59-48fb-b133-d1166490892f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
