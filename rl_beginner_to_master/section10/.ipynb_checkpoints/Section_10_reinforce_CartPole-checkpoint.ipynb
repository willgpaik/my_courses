{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WiKdKQg5of1O",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "<div style=\"text-align:center\">\n",
    "    <h1>\n",
    "        REINFORCE\n",
    "    </h1>\n",
    "</div>\n",
    "\n",
    "<br><br>\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "In this notebook we are going to implement the Monte Carlo version of Policy Gradient methods. The REINFORCE algorithm uses the full return to update the policy:\n",
    "</div>\n",
    "\n",
    "\\begin{equation}\n",
    "G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+1} + \\cdots + \\gamma^{T-t-1} R_{T}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "cellView": "form",
    "id": "fA9rToUrt6Ur"
   },
   "outputs": [],
   "source": [
    "# @title Setup code (not important) - Run this cell by pressing \"Shift + Enter\"\n",
    "\n",
    "\n",
    "\n",
    "#!pip install -qq gym==0.23.0\n",
    "\n",
    "\n",
    "from typing import Tuple, Dict, Optional, Iterable, Callable\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import torch\n",
    "from matplotlib import animation\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from gymnasium.error import DependencyNotInstalled\n",
    "\n",
    "import pygame\n",
    "from pygame import gfxdraw\n",
    "\n",
    "\n",
    "class Maze(gym.Env):\n",
    "\n",
    "    def __init__(self, exploring_starts: bool = False,\n",
    "                 shaped_rewards: bool = False, size: int = 5) -> None:\n",
    "        super().__init__()\n",
    "        self.exploring_starts = exploring_starts\n",
    "        self.shaped_rewards = shaped_rewards\n",
    "        self.state = (size - 1, size - 1)\n",
    "        self.goal = (size - 1, size - 1)\n",
    "        self.maze = self._create_maze(size=size)\n",
    "        self.distances = self._compute_distances(self.goal, self.maze)\n",
    "        self.action_space = spaces.Discrete(n=4)\n",
    "        self.action_space.action_meanings = {0: 'UP', 1: 'RIGHT', 2: 'DOWN', 3: \"LEFT\"}\n",
    "        self.observation_space = spaces.MultiDiscrete([size, size])\n",
    "\n",
    "        self.screen = None\n",
    "        self.agent_transform = None\n",
    "\n",
    "    def step(self, action: int) -> Tuple[Tuple[int, int], float, bool, Dict]:\n",
    "        reward = self.compute_reward(self.state, action)\n",
    "        self.state = self._get_next_state(self.state, action)\n",
    "        done = self.state == self.goal\n",
    "        info = {}\n",
    "        return self.state, reward, done, info\n",
    "\n",
    "    def reset(self) -> Tuple[int, int]:\n",
    "        if self.exploring_starts:\n",
    "            while self.state == self.goal:\n",
    "                self.state = tuple(self.observation_space.sample())\n",
    "        else:\n",
    "            self.state = (0, 0)\n",
    "        return self.state\n",
    "\n",
    "    def render(self, mode: str = 'human') -> Optional[np.ndarray]:\n",
    "        assert mode in ['human', 'rgb_array']\n",
    "\n",
    "        screen_size = 600\n",
    "        scale = screen_size / 5\n",
    "\n",
    "        if self.screen is None:\n",
    "            pygame.init()\n",
    "            self.screen = pygame.Surface((screen_size, screen_size))\n",
    "\n",
    "        surf = pygame.Surface((screen_size, screen_size))\n",
    "        surf.fill((22, 36, 71))\n",
    "\n",
    "\n",
    "        for row in range(5):\n",
    "            for col in range(5):\n",
    "\n",
    "                state = (row, col)\n",
    "                for next_state in [(row + 1, col), (row - 1, col), (row, col + 1), (row, col - 1)]:\n",
    "                    if next_state not in self.maze[state]:\n",
    "\n",
    "                        # Add the geometry of the edges and walls (i.e. the boundaries between\n",
    "                        # adjacent squares that are not connected).\n",
    "                        row_diff, col_diff = np.subtract(next_state, state)\n",
    "                        left = (col + (col_diff > 0)) * scale - 2 * (col_diff != 0)\n",
    "                        right = ((col + 1) - (col_diff < 0)) * scale + 2 * (col_diff != 0)\n",
    "                        top = (5 - (row + (row_diff > 0))) * scale - 2 * (row_diff != 0)\n",
    "                        bottom = (5 - ((row + 1) - (row_diff < 0))) * scale + 2 * (row_diff != 0)\n",
    "\n",
    "                        gfxdraw.filled_polygon(surf, [(left, bottom), (left, top), (right, top), (right, bottom)], (255, 255, 255))\n",
    "\n",
    "        # Add the geometry of the goal square to the viewer.\n",
    "        left, right, top, bottom = scale * 4 + 10, scale * 5 - 10, scale - 10, 10\n",
    "        gfxdraw.filled_polygon(surf, [(left, bottom), (left, top), (right, top), (right, bottom)], (40, 199, 172))\n",
    "\n",
    "        # Add the geometry of the agent to the viewer.\n",
    "        agent_row = int(screen_size - scale * (self.state[0] + .5))\n",
    "        agent_col = int(scale * (self.state[1] + .5))\n",
    "        gfxdraw.filled_circle(surf, agent_col, agent_row, int(scale * .6 / 2), (228, 63, 90))\n",
    "\n",
    "        surf = pygame.transform.flip(surf, False, True)\n",
    "        self.screen.blit(surf, (0, 0))\n",
    "\n",
    "        return np.transpose(\n",
    "                np.array(pygame.surfarray.pixels3d(self.screen)), axes=(1, 0, 2)\n",
    "            )\n",
    "\n",
    "    def close(self) -> None:\n",
    "        if self.screen is not None:\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()\n",
    "            self.screen = None\n",
    "\n",
    "    def compute_reward(self, state: Tuple[int, int], action: int) -> float:\n",
    "        next_state = self._get_next_state(state, action)\n",
    "        if self.shaped_rewards:\n",
    "            return - (self.distances[next_state] / self.distances.max())\n",
    "        return - float(state != self.goal)\n",
    "\n",
    "    def simulate_step(self, state: Tuple[int, int], action: int):\n",
    "        reward = self.compute_reward(state, action)\n",
    "        next_state = self._get_next_state(state, action)\n",
    "        done = next_state == self.goal\n",
    "        info = {}\n",
    "        return next_state, reward, done, info\n",
    "\n",
    "    def _get_next_state(self, state: Tuple[int, int], action: int) -> Tuple[int, int]:\n",
    "        if action == 0:\n",
    "            next_state = (state[0] - 1, state[1])\n",
    "        elif action == 1:\n",
    "            next_state = (state[0], state[1] + 1)\n",
    "        elif action == 2:\n",
    "            next_state = (state[0] + 1, state[1])\n",
    "        elif action == 3:\n",
    "            next_state = (state[0], state[1] - 1)\n",
    "        else:\n",
    "            raise ValueError(\"Action value not supported:\", action)\n",
    "        if next_state in self.maze[state]:\n",
    "            return next_state\n",
    "        return state\n",
    "\n",
    "    @staticmethod\n",
    "    def _create_maze(size: int) -> Dict[Tuple[int, int], Iterable[Tuple[int, int]]]:\n",
    "        maze = {(row, col): [(row - 1, col), (row + 1, col), (row, col - 1), (row, col + 1)]\n",
    "                for row in range(size) for col in range(size)}\n",
    "\n",
    "        left_edges = [[(row, 0), (row, -1)] for row in range(size)]\n",
    "        right_edges = [[(row, size - 1), (row, size)] for row in range(size)]\n",
    "        upper_edges = [[(0, col), (-1, col)] for col in range(size)]\n",
    "        lower_edges = [[(size - 1, col), (size, col)] for col in range(size)]\n",
    "        walls = [\n",
    "            [(1, 0), (1, 1)], [(2, 0), (2, 1)], [(3, 0), (3, 1)],\n",
    "            [(1, 1), (1, 2)], [(2, 1), (2, 2)], [(3, 1), (3, 2)],\n",
    "            [(3, 1), (4, 1)], [(0, 2), (1, 2)], [(1, 2), (1, 3)],\n",
    "            [(2, 2), (3, 2)], [(2, 3), (3, 3)], [(2, 4), (3, 4)],\n",
    "            [(4, 2), (4, 3)], [(1, 3), (1, 4)], [(2, 3), (2, 4)],\n",
    "        ]\n",
    "\n",
    "        obstacles = upper_edges + lower_edges + left_edges + right_edges + walls\n",
    "\n",
    "        for src, dst in obstacles:\n",
    "            maze[src].remove(dst)\n",
    "\n",
    "            if dst in maze:\n",
    "                maze[dst].remove(src)\n",
    "\n",
    "        return maze\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_distances(goal: Tuple[int, int],\n",
    "                           maze: Dict[Tuple[int, int], Iterable[Tuple[int, int]]]) -> np.ndarray:\n",
    "        distances = np.full((5, 5), np.inf)\n",
    "        visited = set()\n",
    "        distances[goal] = 0.\n",
    "\n",
    "        while visited != set(maze):\n",
    "            sorted_dst = [(v // 5, v % 5) for v in distances.argsort(axis=None)]\n",
    "            closest = next(x for x in sorted_dst if x not in visited)\n",
    "            visited.add(closest)\n",
    "\n",
    "            for neighbour in maze[closest]:\n",
    "                distances[neighbour] = min(distances[neighbour], distances[closest] + 1)\n",
    "        return distances\n",
    "\n",
    "\n",
    "def display_video(frames):\n",
    "    # Copied from: https://colab.research.google.com/github/deepmind/dm_control/blob/master/tutorial.ipynb\n",
    "    orig_backend = matplotlib.get_backend()\n",
    "    matplotlib.use('Agg')\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "    matplotlib.use(orig_backend)\n",
    "    ax.set_axis_off()\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_position([0, 0, 1, 1])\n",
    "    im = ax.imshow(frames[0])\n",
    "    def update(frame):\n",
    "        im.set_data(frame)\n",
    "        return [im]\n",
    "    anim = animation.FuncAnimation(fig=fig, func=update, frames=frames,\n",
    "                                    interval=50, blit=True, repeat=False)\n",
    "    return HTML(anim.to_html5_video())\n",
    "\n",
    "\n",
    "def seed_everything(env: gym.Env, seed: int = 42) -> None:\n",
    "    #env.seed(seed)\n",
    "    env.reset(seed=seed)\n",
    "    env.action_space.seed(seed)\n",
    "    env.observation_space.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "\n",
    "\n",
    "def plot_stats(stats):\n",
    "    rows = len(stats)\n",
    "    cols = 1\n",
    "\n",
    "    fig, ax = plt.subplots(rows, cols, figsize=(12, 6))\n",
    "\n",
    "    for i, key in enumerate(stats):\n",
    "        vals = stats[key]\n",
    "        vals = [np.mean(vals[i-10:i+10]) for i in range(10, len(vals)-10)]\n",
    "        if len(stats) > 1:\n",
    "            ax[i].plot(range(len(vals)), vals)\n",
    "            ax[i].set_title(key, size=18)\n",
    "        else:\n",
    "            ax.plot(range(len(vals)), vals)\n",
    "            ax.set_title(key, size=18)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def test_policy_network(env, policy, episodes=10):\n",
    "    frames = []\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        frames.append(env.render())\n",
    "\n",
    "        while not done:\n",
    "            state = torch.from_numpy(state).unsqueeze(0).float()\n",
    "            action = policy(state).multinomial(1).item()\n",
    "            next_state, _, done, _ = env.step(action)\n",
    "            img = env.render()\n",
    "            frames.append(img)\n",
    "            state = next_state\n",
    "\n",
    "    return display_video(frames)\n",
    "\n",
    "\n",
    "def plot_action_probs(probs, labels):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.bar(labels, probs, color ='orange')\n",
    "    plt.title(r\"$\\pi(s)$\", size=16)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "58tdSeQDof1Q"
   },
   "source": [
    "## Import the necessary software libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Eyg1CYzDof1Q"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch import nn as nn\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iZW8fNabof1R"
   },
   "source": [
    "## Create and preprocess the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CLRbdXeHof1R"
   },
   "source": [
    "### Create the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "lyeak161of1R"
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1', render_mode='rgb_array')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "IoMMtfhnof1R"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State dimensions: 4. Actions: 2\n",
      "Sample state: (array([ 0.03720018, -0.00248184, -0.01793681, -0.00461568], dtype=float32), {})\n"
     ]
    }
   ],
   "source": [
    "dims = env.observation_space.shape[0]\n",
    "actions = env.action_space.n\n",
    "\n",
    "print(f\"State dimensions: {dims}. Actions: {actions}\")\n",
    "print(f\"Sample state: {env.reset()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "STG_uBytof1S"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f08a4b13f50>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(env.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dXMIzXMBof1S"
   },
   "source": [
    "### Prepare the environment to work with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "dDLk5_ILof1S"
   },
   "outputs": [],
   "source": [
    "class PreprocessEnv(gym.Wrapper):\n",
    "\n",
    "    def __init__(self, env):\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "\n",
    "    def reset(self):\n",
    "        state, _ = self.env.reset()\n",
    "        print(state)\n",
    "        return torch.from_numpy(state).float()\n",
    "\n",
    "    def step(self, actions):\n",
    "        actions = actions.squeeze().numpy()\n",
    "        next_state, reward, done, info = self.env.step(actions)\n",
    "        next_state = torch.from_numpy(next_state).float()\n",
    "        reward = torch.tensor(reward).unsqueeze(1).float()\n",
    "        done = torch.tensor(done).unsqueeze(1)\n",
    "        return next_state, reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "v0mZg0m8of1S"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SyncVectorEnv' object has no attribute 'seed'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m     env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCartPole-v1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PreprocessEnv(env)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mseed_everything\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparallel_env\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#parallel_env = PreprocessEnv(parallel_env)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m parallel_env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mvector\u001b[38;5;241m.\u001b[39mSyncVectorEnv([make_env] \u001b[38;5;241m*\u001b[39m num_envs)\n",
      "Cell \u001b[0;32mIn[73], line 215\u001b[0m, in \u001b[0;36mseed_everything\u001b[0;34m(env, seed)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mseed_everything\u001b[39m(env, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m):\n\u001b[1;32m    212\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;124;03m    Apply the same seed to all environments in a vectorized environment.\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 215\u001b[0m     \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseed\u001b[49m(seed)\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(env\u001b[38;5;241m.\u001b[39menvs)):\n\u001b[1;32m    217\u001b[0m         env\u001b[38;5;241m.\u001b[39menvs[env_id]\u001b[38;5;241m.\u001b[39mseed(seed \u001b[38;5;241m+\u001b[39m env_id)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SyncVectorEnv' object has no attribute 'seed'"
     ]
    }
   ],
   "source": [
    "num_envs = os.cpu_count()\n",
    "#parallel_env = gym.vector.make('CartPole-v1', num_envs=num_envs)\n",
    "parallel_env = gym.make_vec('CartPole-v1', num_envs=num_envs, vectorization_mode=\"async\")\n",
    "def make_env():\n",
    "    env = gym.make('CartPole-v1')\n",
    "    return PreprocessEnv(env)\n",
    "seed_everything(parallel_env)\n",
    "#parallel_env = PreprocessEnv(parallel_env)\n",
    "parallel_env = gym.vector.AsyncVectorEnv([make_env] * num_envs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "bHzrCzr41tvu"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "PreprocessEnv.reset() got an unexpected keyword argument 'seed'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mparallel_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.12/site-packages/gymnasium/vector/sync_vector_env.py:183\u001b[0m, in \u001b[0;36mSyncVectorEnv.reset\u001b[0;34m(self, seed, options)\u001b[0m\n\u001b[1;32m    181\u001b[0m observations, infos \u001b[38;5;241m=\u001b[39m [], {}\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (env, single_seed) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs, seed)):\n\u001b[0;32m--> 183\u001b[0m     env_obs, env_info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msingle_seed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m     observations\u001b[38;5;241m.\u001b[39mappend(env_obs)\n\u001b[1;32m    186\u001b[0m     infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_info(infos, env_info, i)\n",
      "\u001b[0;31mTypeError\u001b[0m: PreprocessEnv.reset() got an unexpected keyword argument 'seed'"
     ]
    }
   ],
   "source": [
    "parallel_env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N3GqPUywof1T"
   },
   "source": [
    "### Create the policy $\\pi(s)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uvlIB8IWof1T"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KHy8zRU2of1T"
   },
   "source": [
    "### Plot action probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1nLjzrVeof1T"
   },
   "outputs": [],
   "source": [
    "neutral_state = torch.zeros(4)\n",
    "left_danger = torch.tensor([-2.3, 0., 0., 0.])\n",
    "right_danger = torch.tensor([2.3, 0., 0., 0.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hx-4cSAsof1T"
   },
   "source": [
    "#### Plot a neutral environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sj3OyadWof1T"
   },
   "outputs": [],
   "source": [
    "probs = policy(neutral_state).detach().numpy()\n",
    "plot_action_probs(probs=probs, labels=['Move Left', 'Move Right'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gT_UZihBof1T"
   },
   "source": [
    "#### Plot a state where the cart is too far left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I2AZjqgXof1T"
   },
   "outputs": [],
   "source": [
    "probs = policy(left_danger).detach().numpy()\n",
    "plot_action_probs(probs=probs, labels=['Move Left', 'Move Right'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "enPvoiN4of1T"
   },
   "source": [
    "#### Plot a state where the cart is too far right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Q9JOaULof1T"
   },
   "outputs": [],
   "source": [
    "probs = policy(right_danger).detach().numpy()\n",
    "plot_action_probs(probs=probs, labels=['Left', 'Right'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J3ATWKTvof1U"
   },
   "source": [
    "## Implement the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gYnoP-xAof1U"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZyBJEEE1of1U",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pZWJ9j3_of1U"
   },
   "source": [
    "## Show results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DS_5_VWQof1U"
   },
   "source": [
    "### Show execution stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X-dFyu2Vof1U"
   },
   "outputs": [],
   "source": [
    "plot_stats(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NjgLWZNoof1U"
   },
   "source": [
    "### Plot action probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nd801P4yof1U"
   },
   "source": [
    "#### Plot a neutral environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Jn-ITyZof1U"
   },
   "outputs": [],
   "source": [
    "probs = policy(neutral_state).detach().numpy()\n",
    "plot_action_probs(probs=probs, labels=['Move Left', 'Move Right'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dvmD5p2nof1U"
   },
   "source": [
    "#### Plot a state where the cart is too far left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KtjJlWU0of1U"
   },
   "outputs": [],
   "source": [
    "probs = policy(left_danger).detach().numpy()\n",
    "plot_action_probs(probs=probs, labels=['Move Left', 'Move Right'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCzk_4vjof1V"
   },
   "source": [
    "#### Plot a state where the cart is too far right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fv0pfx1bof1V"
   },
   "outputs": [],
   "source": [
    "probs = policy(right_danger).detach().numpy()\n",
    "plot_action_probs(probs=probs, labels=['Left', 'Right'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5svJMeVFof1b"
   },
   "source": [
    "### Test the resulting agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lr0ZAk34of1b"
   },
   "outputs": [],
   "source": [
    "test_policy_network(env, policy, episodes=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YTcgbmLAof1b"
   },
   "source": [
    "## Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xBTW4uJrof1c"
   },
   "source": [
    "[[1] Reinforcement Learning: An Introduction. Ch.13](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
