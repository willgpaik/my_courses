{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gsN12V6QmGO4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "<div style=\"text-align:center\">\n",
    "    <h1>\n",
    "        Deep Q-Learning\n",
    "    </h1>\n",
    "</div>\n",
    "\n",
    "<br><br>\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "\n",
    "In this notebook, we extend the Q-Learning algorithm to use function approximators (Neural Networks). The resulting algorithm is known as Deep Q-Learning.\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "E4pxsu9-mPGv"
   },
   "outputs": [],
   "source": [
    "# @title Setup code (not important) - Run this cell by pressing \"Shift + Enter\"\n",
    "\n",
    "\n",
    "\n",
    "#!pip install -qq gym==0.23.0\n",
    "\n",
    "\n",
    "from typing import Tuple, Dict, Optional, Iterable, Callable\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import torch\n",
    "from matplotlib import animation\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.error import DependencyNotInstalled\n",
    "\n",
    "import pygame\n",
    "from pygame import gfxdraw\n",
    "\n",
    "\n",
    "class Maze(gym.Env):\n",
    "\n",
    "    def __init__(self, exploring_starts: bool = False,\n",
    "                 shaped_rewards: bool = False, size: int = 5) -> None:\n",
    "        super().__init__()\n",
    "        self.exploring_starts = exploring_starts\n",
    "        self.shaped_rewards = shaped_rewards\n",
    "        self.state = (size - 1, size - 1)\n",
    "        self.goal = (size - 1, size - 1)\n",
    "        self.maze = self._create_maze(size=size)\n",
    "        self.distances = self._compute_distances(self.goal, self.maze)\n",
    "        self.action_space = spaces.Discrete(n=4)\n",
    "        self.action_space.action_meanings = {0: 'UP', 1: 'RIGHT', 2: 'DOWN', 3: \"LEFT\"}\n",
    "        self.observation_space = spaces.MultiDiscrete([size, size])\n",
    "\n",
    "        self.screen = None\n",
    "        self.agent_transform = None\n",
    "\n",
    "    def step(self, action: int) -> Tuple[Tuple[int, int], float, bool, Dict]:\n",
    "        reward = self.compute_reward(self.state, action)\n",
    "        self.state = self._get_next_state(self.state, action)\n",
    "        done = self.state == self.goal\n",
    "        info = {}\n",
    "        return self.state, reward, done, info\n",
    "\n",
    "    def reset(self) -> Tuple[int, int]:\n",
    "        if self.exploring_starts:\n",
    "            while self.state == self.goal:\n",
    "                self.state = tuple(self.observation_space.sample())\n",
    "        else:\n",
    "            self.state = (0, 0)\n",
    "        return self.state\n",
    "\n",
    "    def render(self, mode: str = 'human') -> Optional[np.ndarray]:\n",
    "        assert mode in ['human', 'rgb_array']\n",
    "\n",
    "        screen_size = 600\n",
    "        scale = screen_size / 5\n",
    "\n",
    "        if self.screen is None:\n",
    "            pygame.init()\n",
    "            self.screen = pygame.Surface((screen_size, screen_size))\n",
    "\n",
    "        surf = pygame.Surface((screen_size, screen_size))\n",
    "        surf.fill((22, 36, 71))\n",
    "\n",
    "\n",
    "        for row in range(5):\n",
    "            for col in range(5):\n",
    "\n",
    "                state = (row, col)\n",
    "                for next_state in [(row + 1, col), (row - 1, col), (row, col + 1), (row, col - 1)]:\n",
    "                    if next_state not in self.maze[state]:\n",
    "\n",
    "                        # Add the geometry of the edges and walls (i.e. the boundaries between\n",
    "                        # adjacent squares that are not connected).\n",
    "                        row_diff, col_diff = np.subtract(next_state, state)\n",
    "                        left = (col + (col_diff > 0)) * scale - 2 * (col_diff != 0)\n",
    "                        right = ((col + 1) - (col_diff < 0)) * scale + 2 * (col_diff != 0)\n",
    "                        top = (5 - (row + (row_diff > 0))) * scale - 2 * (row_diff != 0)\n",
    "                        bottom = (5 - ((row + 1) - (row_diff < 0))) * scale + 2 * (row_diff != 0)\n",
    "\n",
    "                        gfxdraw.filled_polygon(surf, [(left, bottom), (left, top), (right, top), (right, bottom)], (255, 255, 255))\n",
    "\n",
    "        # Add the geometry of the goal square to the viewer.\n",
    "        left, right, top, bottom = scale * 4 + 10, scale * 5 - 10, scale - 10, 10\n",
    "        gfxdraw.filled_polygon(surf, [(left, bottom), (left, top), (right, top), (right, bottom)], (40, 199, 172))\n",
    "\n",
    "        # Add the geometry of the agent to the viewer.\n",
    "        agent_row = int(screen_size - scale * (self.state[0] + .5))\n",
    "        agent_col = int(scale * (self.state[1] + .5))\n",
    "        gfxdraw.filled_circle(surf, agent_col, agent_row, int(scale * .6 / 2), (228, 63, 90))\n",
    "\n",
    "        surf = pygame.transform.flip(surf, False, True)\n",
    "        self.screen.blit(surf, (0, 0))\n",
    "\n",
    "        return np.transpose(\n",
    "                np.array(pygame.surfarray.pixels3d(self.screen)), axes=(1, 0, 2)\n",
    "            )\n",
    "\n",
    "    def close(self) -> None:\n",
    "        if self.screen is not None:\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()\n",
    "            self.screen = None\n",
    "\n",
    "    def compute_reward(self, state: Tuple[int, int], action: int) -> float:\n",
    "        next_state = self._get_next_state(state, action)\n",
    "        if self.shaped_rewards:\n",
    "            return - (self.distances[next_state] / self.distances.max())\n",
    "        return - float(state != self.goal)\n",
    "\n",
    "    def simulate_step(self, state: Tuple[int, int], action: int):\n",
    "        reward = self.compute_reward(state, action)\n",
    "        next_state = self._get_next_state(state, action)\n",
    "        done = next_state == self.goal\n",
    "        info = {}\n",
    "        return next_state, reward, done, info\n",
    "\n",
    "    def _get_next_state(self, state: Tuple[int, int], action: int) -> Tuple[int, int]:\n",
    "        if action == 0:\n",
    "            next_state = (state[0] - 1, state[1])\n",
    "        elif action == 1:\n",
    "            next_state = (state[0], state[1] + 1)\n",
    "        elif action == 2:\n",
    "            next_state = (state[0] + 1, state[1])\n",
    "        elif action == 3:\n",
    "            next_state = (state[0], state[1] - 1)\n",
    "        else:\n",
    "            raise ValueError(\"Action value not supported:\", action)\n",
    "        if next_state in self.maze[state]:\n",
    "            return next_state\n",
    "        return state\n",
    "\n",
    "    @staticmethod\n",
    "    def _create_maze(size: int) -> Dict[Tuple[int, int], Iterable[Tuple[int, int]]]:\n",
    "        maze = {(row, col): [(row - 1, col), (row + 1, col), (row, col - 1), (row, col + 1)]\n",
    "                for row in range(size) for col in range(size)}\n",
    "\n",
    "        left_edges = [[(row, 0), (row, -1)] for row in range(size)]\n",
    "        right_edges = [[(row, size - 1), (row, size)] for row in range(size)]\n",
    "        upper_edges = [[(0, col), (-1, col)] for col in range(size)]\n",
    "        lower_edges = [[(size - 1, col), (size, col)] for col in range(size)]\n",
    "        walls = [\n",
    "            [(1, 0), (1, 1)], [(2, 0), (2, 1)], [(3, 0), (3, 1)],\n",
    "            [(1, 1), (1, 2)], [(2, 1), (2, 2)], [(3, 1), (3, 2)],\n",
    "            [(3, 1), (4, 1)], [(0, 2), (1, 2)], [(1, 2), (1, 3)],\n",
    "            [(2, 2), (3, 2)], [(2, 3), (3, 3)], [(2, 4), (3, 4)],\n",
    "            [(4, 2), (4, 3)], [(1, 3), (1, 4)], [(2, 3), (2, 4)],\n",
    "        ]\n",
    "\n",
    "        obstacles = upper_edges + lower_edges + left_edges + right_edges + walls\n",
    "\n",
    "        for src, dst in obstacles:\n",
    "            maze[src].remove(dst)\n",
    "\n",
    "            if dst in maze:\n",
    "                maze[dst].remove(src)\n",
    "\n",
    "        return maze\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_distances(goal: Tuple[int, int],\n",
    "                           maze: Dict[Tuple[int, int], Iterable[Tuple[int, int]]]) -> np.ndarray:\n",
    "        distances = np.full((5, 5), np.inf)\n",
    "        visited = set()\n",
    "        distances[goal] = 0.\n",
    "\n",
    "        while visited != set(maze):\n",
    "            sorted_dst = [(v // 5, v % 5) for v in distances.argsort(axis=None)]\n",
    "            closest = next(x for x in sorted_dst if x not in visited)\n",
    "            visited.add(closest)\n",
    "\n",
    "            for neighbour in maze[closest]:\n",
    "                distances[neighbour] = min(distances[neighbour], distances[closest] + 1)\n",
    "        return distances\n",
    "\n",
    "\n",
    "def display_video(frames):\n",
    "    # Copied from: https://colab.research.google.com/github/deepmind/dm_control/blob/master/tutorial.ipynb\n",
    "    orig_backend = matplotlib.get_backend()\n",
    "    matplotlib.use('Agg')\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "    matplotlib.use(orig_backend)\n",
    "    ax.set_axis_off()\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_position([0, 0, 1, 1])\n",
    "    im = ax.imshow(frames[0])\n",
    "    def update(frame):\n",
    "        im.set_data(frame)\n",
    "        return [im]\n",
    "    anim = animation.FuncAnimation(fig=fig, func=update, frames=frames,\n",
    "                                    interval=50, blit=True, repeat=False)\n",
    "    return HTML(anim.to_html5_video())\n",
    "\n",
    "\n",
    "def test_agent(env, policy, episodes=10):\n",
    "    frames = []\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        frames.append(env.render(mode=\"rgb_array\"))\n",
    "\n",
    "        while not done:\n",
    "            p = policy(state)\n",
    "            if isinstance(p, np.ndarray):\n",
    "                action = np.random.choice(4, p=p)\n",
    "            else:\n",
    "                action = p\n",
    "            next_state, reward, done, extra_info = env.step(action)\n",
    "            img = env.render(mode=\"rgb_array\")\n",
    "            frames.append(img)\n",
    "            state = next_state\n",
    "\n",
    "    return display_video(frames)\n",
    "\n",
    "\n",
    "def seed_everything(env: gym.Env, seed: int = 42) -> None:\n",
    "    env.seed(seed)\n",
    "    env.action_space.seed(seed)\n",
    "    env.observation_space.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "\n",
    "\n",
    "def plot_stats(stats):\n",
    "    rows = len(stats)\n",
    "    cols = 1\n",
    "\n",
    "    fig, ax = plt.subplots(rows, cols, figsize=(12, 6))\n",
    "\n",
    "    for i, key in enumerate(stats):\n",
    "        vals = stats[key]\n",
    "        vals = [np.mean(vals[i-10:i+10]) for i in range(10, len(vals)-10)]\n",
    "        if len(stats) > 1:\n",
    "            ax[i].plot(range(len(vals)), vals)\n",
    "            ax[i].set_title(key, size=18)\n",
    "        else:\n",
    "            ax.plot(range(len(vals)), vals)\n",
    "            ax.set_title(key, size=18)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_cost_to_go(env, q_network, xlabel=None, ylabel=None):\n",
    "    highx, highy = env.observation_space.high\n",
    "    lowx, lowy = env.observation_space.low\n",
    "    X = torch.linspace(lowx, highx, 100)\n",
    "    Y = torch.linspace(lowy, highy, 100)\n",
    "    X, Y = torch.meshgrid(X, Y)\n",
    "\n",
    "    q_net_input = torch.stack([X.flatten(), Y.flatten()], dim=-1)\n",
    "    Z = - q_network(q_net_input).max(dim=-1, keepdim=True)[0]\n",
    "    Z = Z.reshape(100, 100).detach().numpy()\n",
    "    X = X.numpy()\n",
    "    Y = Y.numpy()\n",
    "\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    surf = ax.plot_surface(X, Y, Z, cmap='jet', linewidth=0, antialiased=False)\n",
    "    fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "    ax.set_xlabel(xlabel, size=14)\n",
    "    ax.set_ylabel(ylabel, size=14)\n",
    "    ax.set_title(\"Estimated cost-to-go\", size=18)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_max_q(env, q_network, xlabel=None, ylabel=None, action_labels=[]):\n",
    "    highx, highy = env.observation_space.high\n",
    "    lowx, lowy = env.observation_space.low\n",
    "    X = torch.linspace(lowx, highx, 100)\n",
    "    Y = torch.linspace(lowy, highy, 100)\n",
    "    X, Y = torch.meshgrid(X, Y)\n",
    "    q_net_input = torch.stack([X.flatten(), Y.flatten()], dim=-1)\n",
    "    Z = q_network(q_net_input).argmax(dim=-1, keepdim=True)\n",
    "    Z = Z.reshape(100, 100).T.detach().numpy()\n",
    "    values = np.unique(Z.ravel())\n",
    "    values.sort()\n",
    "\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.xlabel(xlabel, size=14)\n",
    "    plt.ylabel(ylabel, size=14)\n",
    "    plt.title(\"Optimal action\", size=18)\n",
    "\n",
    "    im = plt.imshow(Z, cmap='jet')\n",
    "    colors = [im.cmap(im.norm(value)) for value in values]\n",
    "    patches = [mpatches.Patch(color=color, label=label) for color, label in zip(colors, action_labels)]\n",
    "    plt.legend(handles=patches, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QXh1rh7YmGO7"
   },
   "source": [
    "## Import the necessary software libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "yIz69YQomGO7"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import copy\n",
    "import gymnasium  as gym\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn as nn\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7CYkeXJCmGO8"
   },
   "source": [
    "## Create and prepare the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PV0R_SojmGO8"
   },
   "source": [
    "### Create the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "fCeFY5FamGO8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:488: RuntimeWarning: Your system is avx2 capable but pygame was not built with support for it. The performance of some of your blits could be adversely affected. Consider enabling compile time detection with environment variables like PYGAME_DETECT_AVX2=1 if you are compiling without cross compilation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc1e7583170>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0', render_mode='rgb_array')\n",
    "env.reset()\n",
    "plt.imshow(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qi2Tk4qumGO9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8lRro8JfmGO9"
   },
   "source": [
    "### Prepare the environment to work with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tr4VrlvjmGO9"
   },
   "outputs": [],
   "source": [
    "class PreprocessEnv(gym.Wrapper):\n",
    "\n",
    "    def __init__(self, env):\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "\n",
    "    def reset(self):\n",
    "        obs = self.env.reset()\n",
    "        return torch.from_numpy(obs).unsqueeze(dim=0).float()\n",
    "\n",
    "    def step(self, action):\n",
    "        action = action.item()\n",
    "        next_state, reward, done, info = self.env.step(action)\n",
    "        next_state = torch.from_numpy(next_state).unsqueeze(dim=0).float()\n",
    "        reward = torch.tensor(reward).view(1, -1).float()\n",
    "        done = torch.tensor(done).view(1, -1)\n",
    "        return next_state, reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v87pAharmGO9"
   },
   "outputs": [],
   "source": [
    "env = PreprocessEnv(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "82nlQI26mGO9"
   },
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "action = torch.tensor(0)\n",
    "next_state, reward, done, _ = env.step(action)\n",
    "print(f\"Sample state: {state}\")\n",
    "print(f\"Next state: {next_state}, Reward: {reward}, Done: {done}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_aDkOllmmGO9"
   },
   "source": [
    "## Create the Q-Network and policy\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7lPti0REmGO-"
   },
   "source": [
    "### Create the Q-Network: $\\hat q(s,a| \\theta)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ABFi9TU9mGO-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PH4vUbtUmGO-"
   },
   "source": [
    "### Create the target Q-Network: $\\hat q(s, a|\\theta_{targ})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RGqbOfrEmGO-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FJ1lw7UymGO-"
   },
   "source": [
    "### Create the exploratory policy: $b(s)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T6E9kafymGO-"
   },
   "outputs": [],
   "source": [
    "def policy(state, epsilon=0.):\n",
    "    if torch.rand(1) < epsilon:\n",
    "        return torch.randint(num_actions, (1, 1))\n",
    "    else:\n",
    "        av = q_network(state).detach()\n",
    "        return torch.argmax(av, dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dVHtSSQymGO-"
   },
   "source": [
    "## Create the Experience Replay buffer\n",
    "\n",
    "<br>\n",
    "<div style=\"text-align:center\">\n",
    "    <p>A simple buffer that stores transitions of arbitrary values, adapted from\n",
    "    <a href=\"https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html#training\">this source.</a></p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FjpSAtp4mGO-"
   },
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "\n",
    "    def __init__(self, capacity=100000):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def insert(self, transition):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = transition\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        assert self.can_sample(batch_size)\n",
    "\n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        batch = zip(*batch)\n",
    "        return [torch.cat(items) for items in batch]\n",
    "\n",
    "    def can_sample(self, batch_size):\n",
    "        return len(self.memory) >= batch_size * 10\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pfIPpqjJmGO-",
    "scrolled": false
   },
   "source": [
    "## Implement the algorithm\n",
    "\n",
    "</br></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1VxwD6svmGO_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kbi8PEzlmGO_",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p-3XR-_OmGO_"
   },
   "source": [
    "## Show results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VCMwXERHmGO_"
   },
   "source": [
    "### Plot execution stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kWrIfjzkmGO_"
   },
   "outputs": [],
   "source": [
    "plot_stats(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SpLObvs5mGO_"
   },
   "source": [
    "### Test the resulting agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LKyHEYyemGO_"
   },
   "outputs": [],
   "source": [
    "test_agent(env, policy, episodes=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KK-lKC9RmGO_"
   },
   "source": [
    "## Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5y5r_670mGO_"
   },
   "source": [
    "[[1] Playing Atari with Deep Reinforcement Learning](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
