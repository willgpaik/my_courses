{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "WiKdKQg5of1O"
      },
      "source": [
        "<div style=\"text-align:center\">\n",
        "    <h1>\n",
        "        REINFORCE\n",
        "    </h1>\n",
        "</div>\n",
        "\n",
        "<br><br>\n",
        "\n",
        "<div style=\"text-align:center\">\n",
        "In this notebook we are going to implement the Monte Carlo version of Policy Gradient methods. The REINFORCE algorithm uses the full return to update the policy:\n",
        "</div>\n",
        "\n",
        "\\begin{equation}\n",
        "G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+1} + \\cdots + \\gamma^{T-t-1} R_{T}\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Setup code (not important) - Run this cell by pressing \"Shift + Enter\"\n",
        "\n",
        "\n",
        "\n",
        "!pip install -qq gym==0.23.0\n",
        "\n",
        "\n",
        "from typing import Tuple, Dict, Optional, Iterable, Callable\n",
        "\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib\n",
        "import torch\n",
        "from matplotlib import animation\n",
        "import matplotlib.patches as mpatches\n",
        "\n",
        "from IPython.display import HTML\n",
        "\n",
        "import gym\n",
        "from gym import spaces\n",
        "from gym.error import DependencyNotInstalled\n",
        "\n",
        "import pygame\n",
        "from pygame import gfxdraw\n",
        "\n",
        "\n",
        "class Maze(gym.Env):\n",
        "\n",
        "    def __init__(self, exploring_starts: bool = False,\n",
        "                 shaped_rewards: bool = False, size: int = 5) -> None:\n",
        "        super().__init__()\n",
        "        self.exploring_starts = exploring_starts\n",
        "        self.shaped_rewards = shaped_rewards\n",
        "        self.state = (size - 1, size - 1)\n",
        "        self.goal = (size - 1, size - 1)\n",
        "        self.maze = self._create_maze(size=size)\n",
        "        self.distances = self._compute_distances(self.goal, self.maze)\n",
        "        self.action_space = spaces.Discrete(n=4)\n",
        "        self.action_space.action_meanings = {0: 'UP', 1: 'RIGHT', 2: 'DOWN', 3: \"LEFT\"}\n",
        "        self.observation_space = spaces.MultiDiscrete([size, size])\n",
        "\n",
        "        self.screen = None\n",
        "        self.agent_transform = None\n",
        "\n",
        "    def step(self, action: int) -> Tuple[Tuple[int, int], float, bool, Dict]:\n",
        "        reward = self.compute_reward(self.state, action)\n",
        "        self.state = self._get_next_state(self.state, action)\n",
        "        done = self.state == self.goal\n",
        "        info = {}\n",
        "        return self.state, reward, done, info\n",
        "\n",
        "    def reset(self) -> Tuple[int, int]:\n",
        "        if self.exploring_starts:\n",
        "            while self.state == self.goal:\n",
        "                self.state = tuple(self.observation_space.sample())\n",
        "        else:\n",
        "            self.state = (0, 0)\n",
        "        return self.state\n",
        "\n",
        "    def render(self, mode: str = 'human') -> Optional[np.ndarray]:\n",
        "        assert mode in ['human', 'rgb_array']\n",
        "\n",
        "        screen_size = 600\n",
        "        scale = screen_size / 5\n",
        "\n",
        "        if self.screen is None:\n",
        "            pygame.init()\n",
        "            self.screen = pygame.Surface((screen_size, screen_size))\n",
        "\n",
        "        surf = pygame.Surface((screen_size, screen_size))\n",
        "        surf.fill((22, 36, 71))\n",
        "\n",
        "\n",
        "        for row in range(5):\n",
        "            for col in range(5):\n",
        "\n",
        "                state = (row, col)\n",
        "                for next_state in [(row + 1, col), (row - 1, col), (row, col + 1), (row, col - 1)]:\n",
        "                    if next_state not in self.maze[state]:\n",
        "\n",
        "                        # Add the geometry of the edges and walls (i.e. the boundaries between\n",
        "                        # adjacent squares that are not connected).\n",
        "                        row_diff, col_diff = np.subtract(next_state, state)\n",
        "                        left = (col + (col_diff > 0)) * scale - 2 * (col_diff != 0)\n",
        "                        right = ((col + 1) - (col_diff < 0)) * scale + 2 * (col_diff != 0)\n",
        "                        top = (5 - (row + (row_diff > 0))) * scale - 2 * (row_diff != 0)\n",
        "                        bottom = (5 - ((row + 1) - (row_diff < 0))) * scale + 2 * (row_diff != 0)\n",
        "\n",
        "                        gfxdraw.filled_polygon(surf, [(left, bottom), (left, top), (right, top), (right, bottom)], (255, 255, 255))\n",
        "\n",
        "        # Add the geometry of the goal square to the viewer.\n",
        "        left, right, top, bottom = scale * 4 + 10, scale * 5 - 10, scale - 10, 10\n",
        "        gfxdraw.filled_polygon(surf, [(left, bottom), (left, top), (right, top), (right, bottom)], (40, 199, 172))\n",
        "\n",
        "        # Add the geometry of the agent to the viewer.\n",
        "        agent_row = int(screen_size - scale * (self.state[0] + .5))\n",
        "        agent_col = int(scale * (self.state[1] + .5))\n",
        "        gfxdraw.filled_circle(surf, agent_col, agent_row, int(scale * .6 / 2), (228, 63, 90))\n",
        "\n",
        "        surf = pygame.transform.flip(surf, False, True)\n",
        "        self.screen.blit(surf, (0, 0))\n",
        "\n",
        "        return np.transpose(\n",
        "                np.array(pygame.surfarray.pixels3d(self.screen)), axes=(1, 0, 2)\n",
        "            )\n",
        "\n",
        "    def close(self) -> None:\n",
        "        if self.screen is not None:\n",
        "            pygame.display.quit()\n",
        "            pygame.quit()\n",
        "            self.screen = None\n",
        "\n",
        "    def compute_reward(self, state: Tuple[int, int], action: int) -> float:\n",
        "        next_state = self._get_next_state(state, action)\n",
        "        if self.shaped_rewards:\n",
        "            return - (self.distances[next_state] / self.distances.max())\n",
        "        return - float(state != self.goal)\n",
        "\n",
        "    def simulate_step(self, state: Tuple[int, int], action: int):\n",
        "        reward = self.compute_reward(state, action)\n",
        "        next_state = self._get_next_state(state, action)\n",
        "        done = next_state == self.goal\n",
        "        info = {}\n",
        "        return next_state, reward, done, info\n",
        "\n",
        "    def _get_next_state(self, state: Tuple[int, int], action: int) -> Tuple[int, int]:\n",
        "        if action == 0:\n",
        "            next_state = (state[0] - 1, state[1])\n",
        "        elif action == 1:\n",
        "            next_state = (state[0], state[1] + 1)\n",
        "        elif action == 2:\n",
        "            next_state = (state[0] + 1, state[1])\n",
        "        elif action == 3:\n",
        "            next_state = (state[0], state[1] - 1)\n",
        "        else:\n",
        "            raise ValueError(\"Action value not supported:\", action)\n",
        "        if next_state in self.maze[state]:\n",
        "            return next_state\n",
        "        return state\n",
        "\n",
        "    @staticmethod\n",
        "    def _create_maze(size: int) -> Dict[Tuple[int, int], Iterable[Tuple[int, int]]]:\n",
        "        maze = {(row, col): [(row - 1, col), (row + 1, col), (row, col - 1), (row, col + 1)]\n",
        "                for row in range(size) for col in range(size)}\n",
        "\n",
        "        left_edges = [[(row, 0), (row, -1)] for row in range(size)]\n",
        "        right_edges = [[(row, size - 1), (row, size)] for row in range(size)]\n",
        "        upper_edges = [[(0, col), (-1, col)] for col in range(size)]\n",
        "        lower_edges = [[(size - 1, col), (size, col)] for col in range(size)]\n",
        "        walls = [\n",
        "            [(1, 0), (1, 1)], [(2, 0), (2, 1)], [(3, 0), (3, 1)],\n",
        "            [(1, 1), (1, 2)], [(2, 1), (2, 2)], [(3, 1), (3, 2)],\n",
        "            [(3, 1), (4, 1)], [(0, 2), (1, 2)], [(1, 2), (1, 3)],\n",
        "            [(2, 2), (3, 2)], [(2, 3), (3, 3)], [(2, 4), (3, 4)],\n",
        "            [(4, 2), (4, 3)], [(1, 3), (1, 4)], [(2, 3), (2, 4)],\n",
        "        ]\n",
        "\n",
        "        obstacles = upper_edges + lower_edges + left_edges + right_edges + walls\n",
        "\n",
        "        for src, dst in obstacles:\n",
        "            maze[src].remove(dst)\n",
        "\n",
        "            if dst in maze:\n",
        "                maze[dst].remove(src)\n",
        "\n",
        "        return maze\n",
        "\n",
        "    @staticmethod\n",
        "    def _compute_distances(goal: Tuple[int, int],\n",
        "                           maze: Dict[Tuple[int, int], Iterable[Tuple[int, int]]]) -> np.ndarray:\n",
        "        distances = np.full((5, 5), np.inf)\n",
        "        visited = set()\n",
        "        distances[goal] = 0.\n",
        "\n",
        "        while visited != set(maze):\n",
        "            sorted_dst = [(v // 5, v % 5) for v in distances.argsort(axis=None)]\n",
        "            closest = next(x for x in sorted_dst if x not in visited)\n",
        "            visited.add(closest)\n",
        "\n",
        "            for neighbour in maze[closest]:\n",
        "                distances[neighbour] = min(distances[neighbour], distances[closest] + 1)\n",
        "        return distances\n",
        "\n",
        "\n",
        "def display_video(frames):\n",
        "    # Copied from: https://colab.research.google.com/github/deepmind/dm_control/blob/master/tutorial.ipynb\n",
        "    orig_backend = matplotlib.get_backend()\n",
        "    matplotlib.use('Agg')\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
        "    matplotlib.use(orig_backend)\n",
        "    ax.set_axis_off()\n",
        "    ax.set_aspect('equal')\n",
        "    ax.set_position([0, 0, 1, 1])\n",
        "    im = ax.imshow(frames[0])\n",
        "    def update(frame):\n",
        "        im.set_data(frame)\n",
        "        return [im]\n",
        "    anim = animation.FuncAnimation(fig=fig, func=update, frames=frames,\n",
        "                                    interval=50, blit=True, repeat=False)\n",
        "    return HTML(anim.to_html5_video())\n",
        "\n",
        "\n",
        "def seed_everything(env: gym.Env, seed: int = 42) -> None:\n",
        "    env.seed(seed)\n",
        "    env.action_space.seed(seed)\n",
        "    env.observation_space.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.use_deterministic_algorithms(True)\n",
        "\n",
        "\n",
        "def plot_stats(stats):\n",
        "    rows = len(stats)\n",
        "    cols = 1\n",
        "\n",
        "    fig, ax = plt.subplots(rows, cols, figsize=(12, 6))\n",
        "\n",
        "    for i, key in enumerate(stats):\n",
        "        vals = stats[key]\n",
        "        vals = [np.mean(vals[i-10:i+10]) for i in range(10, len(vals)-10)]\n",
        "        if len(stats) > 1:\n",
        "            ax[i].plot(range(len(vals)), vals)\n",
        "            ax[i].set_title(key, size=18)\n",
        "        else:\n",
        "            ax.plot(range(len(vals)), vals)\n",
        "            ax.set_title(key, size=18)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def test_policy_network(env, policy, episodes=10):\n",
        "    frames = []\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        frames.append(env.render(mode=\"rgb_array\"))\n",
        "\n",
        "        while not done:\n",
        "            state = torch.from_numpy(state).unsqueeze(0).float()\n",
        "            action = policy(state).multinomial(1).item()\n",
        "            next_state, _, done, _ = env.step(action)\n",
        "            img = env.render(mode=\"rgb_array\")\n",
        "            frames.append(img)\n",
        "            state = next_state\n",
        "\n",
        "    return display_video(frames)\n",
        "\n",
        "\n",
        "def plot_action_probs(probs, labels):\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.bar(labels, probs, color ='orange')\n",
        "    plt.title(\"$\\pi(s)$\", size=16)\n",
        "    plt.xticks(fontsize=12)\n",
        "    plt.yticks(fontsize=12)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fA9rToUrt6Ur",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58tdSeQDof1Q"
      },
      "source": [
        "## Import the necessary software libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eyg1CYzDof1Q"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from torch import nn as nn\n",
        "from torch.optim import AdamW"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZW8fNabof1R"
      },
      "source": [
        "## Create and preprocess the environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLRbdXeHof1R"
      },
      "source": [
        "### Create the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lyeak161of1R"
      },
      "outputs": [],
      "source": [
        "env = gym.make('CartPole-v0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IoMMtfhnof1R"
      },
      "outputs": [],
      "source": [
        "dims = env.observation_space.shape[0]\n",
        "actions = env.action_space.n\n",
        "\n",
        "print(f\"State dimensions: {dims}. Actions: {actions}\")\n",
        "print(f\"Sample state: {env.reset()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STG_uBytof1S"
      },
      "outputs": [],
      "source": [
        "plt.imshow(env.render(mode='rgb_array'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXMIzXMBof1S"
      },
      "source": [
        "### Prepare the environment to work with PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDLk5_ILof1S"
      },
      "outputs": [],
      "source": [
        "class PreprocessEnv(gym.Wrapper):\n",
        "\n",
        "    def __init__(self, env):\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "\n",
        "    def reset(self):\n",
        "        state = self.env.reset()\n",
        "        return torch.from_numpy(state).float()\n",
        "\n",
        "    def step(self, actions):\n",
        "        actions = actions.squeeze().numpy()\n",
        "        next_state, reward, done, info = self.env.step(actions)\n",
        "        next_state = torch.from_numpy(next_state).float()\n",
        "        reward = torch.tensor(reward).unsqueeze(1).float()\n",
        "        done = torch.tensor(done).unsqueeze(1)\n",
        "        return next_state, reward, done, info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0mZg0m8of1S"
      },
      "outputs": [],
      "source": [
        "num_envs = os.cpu_count()\n",
        "parallel_env = gym.vector.make('CartPole-v1', num_envs=num_envs)\n",
        "seed_everything(parallel_env)\n",
        "parallel_env = PreprocessEnv(parallel_env)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parallel_env.reset()"
      ],
      "metadata": {
        "id": "bHzrCzr41tvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3GqPUywof1T"
      },
      "source": [
        "### Create the policy $\\pi(s)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvlIB8IWof1T"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHy8zRU2of1T"
      },
      "source": [
        "### Plot action probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nLjzrVeof1T"
      },
      "outputs": [],
      "source": [
        "neutral_state = torch.zeros(4)\n",
        "left_danger = torch.tensor([-2.3, 0., 0., 0.])\n",
        "right_danger = torch.tensor([2.3, 0., 0., 0.])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hx-4cSAsof1T"
      },
      "source": [
        "#### Plot a neutral environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sj3OyadWof1T"
      },
      "outputs": [],
      "source": [
        "probs = policy(neutral_state).detach().numpy()\n",
        "plot_action_probs(probs=probs, labels=['Move Left', 'Move Right'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gT_UZihBof1T"
      },
      "source": [
        "#### Plot a state where the cart is too far left"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2AZjqgXof1T"
      },
      "outputs": [],
      "source": [
        "probs = policy(left_danger).detach().numpy()\n",
        "plot_action_probs(probs=probs, labels=['Move Left', 'Move Right'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enPvoiN4of1T"
      },
      "source": [
        "#### Plot a state where the cart is too far right"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Q9JOaULof1T"
      },
      "outputs": [],
      "source": [
        "probs = policy(right_danger).detach().numpy()\n",
        "plot_action_probs(probs=probs, labels=['Left', 'Right'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3ATWKTvof1U"
      },
      "source": [
        "## Implement the algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYnoP-xAof1U"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "ZyBJEEE1of1U"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZWJ9j3_of1U"
      },
      "source": [
        "## Show results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DS_5_VWQof1U"
      },
      "source": [
        "### Show execution stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-dFyu2Vof1U"
      },
      "outputs": [],
      "source": [
        "plot_stats(stats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjgLWZNoof1U"
      },
      "source": [
        "### Plot action probabilities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nd801P4yof1U"
      },
      "source": [
        "#### Plot a neutral environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Jn-ITyZof1U"
      },
      "outputs": [],
      "source": [
        "probs = policy(neutral_state).detach().numpy()\n",
        "plot_action_probs(probs=probs, labels=['Move Left', 'Move Right'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvmD5p2nof1U"
      },
      "source": [
        "#### Plot a state where the cart is too far left"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtjJlWU0of1U"
      },
      "outputs": [],
      "source": [
        "probs = policy(left_danger).detach().numpy()\n",
        "plot_action_probs(probs=probs, labels=['Move Left', 'Move Right'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCzk_4vjof1V"
      },
      "source": [
        "#### Plot a state where the cart is too far right"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fv0pfx1bof1V"
      },
      "outputs": [],
      "source": [
        "probs = policy(right_danger).detach().numpy()\n",
        "plot_action_probs(probs=probs, labels=['Left', 'Right'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5svJMeVFof1b"
      },
      "source": [
        "### Test the resulting agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lr0ZAk34of1b"
      },
      "outputs": [],
      "source": [
        "test_policy_network(env, policy, episodes=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTcgbmLAof1b"
      },
      "source": [
        "## Resources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBTW4uJrof1c"
      },
      "source": [
        "[[1] Reinforcement Learning: An Introduction. Ch.13](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}