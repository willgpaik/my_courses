{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "67rhCAvE7vV4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "<div style=\"text-align:center\">\n",
    "    <h1>\n",
    "        Advantage Actor-Critic (A2C)\n",
    "    </h1>\n",
    "</div>\n",
    "\n",
    "<br><br>\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "In this notebook we are going to combine temporal difference learning (TD) with policy gradient methods. The resulting algorithm is called Advantage Actor-Critic (A2C) and uses a one-step estimate of the return to update the policy:\n",
    "</div>\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat G_t = R_{t+1} + \\gamma v(S_{t+1}|w)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "cellView": "form",
    "id": "_uUI8vNy703R"
   },
   "outputs": [],
   "source": [
    "# @title Setup code (not important) - Run this cell by pressing \"Shift + Enter\"\n",
    "\n",
    "\n",
    "\n",
    "#!pip install -qq gym==0.23.0\n",
    "\n",
    "\n",
    "from typing import Tuple, Dict, Optional, Iterable, Callable\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import torch\n",
    "from matplotlib import animation\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from gymnasium.error import DependencyNotInstalled\n",
    "\n",
    "import pygame\n",
    "from pygame import gfxdraw\n",
    "\n",
    "\n",
    "class Maze(gym.Env):\n",
    "\n",
    "    def __init__(self, exploring_starts: bool = False,\n",
    "                 shaped_rewards: bool = False, size: int = 5) -> None:\n",
    "        super().__init__()\n",
    "        self.exploring_starts = exploring_starts\n",
    "        self.shaped_rewards = shaped_rewards\n",
    "        self.state = (size - 1, size - 1)\n",
    "        self.goal = (size - 1, size - 1)\n",
    "        self.maze = self._create_maze(size=size)\n",
    "        self.distances = self._compute_distances(self.goal, self.maze)\n",
    "        self.action_space = spaces.Discrete(n=4)\n",
    "        self.action_space.action_meanings = {0: 'UP', 1: 'RIGHT', 2: 'DOWN', 3: \"LEFT\"}\n",
    "        self.observation_space = spaces.MultiDiscrete([size, size])\n",
    "\n",
    "        self.screen = None\n",
    "        self.agent_transform = None\n",
    "\n",
    "    def step(self, action: int) -> Tuple[Tuple[int, int], float, bool, Dict]:\n",
    "        reward = self.compute_reward(self.state, action)\n",
    "        self.state = self._get_next_state(self.state, action)\n",
    "        done = self.state == self.goal\n",
    "        info = {}\n",
    "        return self.state, reward, done, info\n",
    "\n",
    "    def reset(self) -> Tuple[int, int]:\n",
    "        if self.exploring_starts:\n",
    "            while self.state == self.goal:\n",
    "                self.state = tuple(self.observation_space.sample())\n",
    "        else:\n",
    "            self.state = (0, 0)\n",
    "        return self.state\n",
    "\n",
    "    def render(self, mode: str = 'human') -> Optional[np.ndarray]:\n",
    "        assert mode in ['human', 'rgb_array']\n",
    "\n",
    "        screen_size = 600\n",
    "        scale = screen_size / 5\n",
    "\n",
    "        if self.screen is None:\n",
    "            pygame.init()\n",
    "            self.screen = pygame.Surface((screen_size, screen_size))\n",
    "\n",
    "        surf = pygame.Surface((screen_size, screen_size))\n",
    "        surf.fill((22, 36, 71))\n",
    "\n",
    "\n",
    "        for row in range(5):\n",
    "            for col in range(5):\n",
    "\n",
    "                state = (row, col)\n",
    "                for next_state in [(row + 1, col), (row - 1, col), (row, col + 1), (row, col - 1)]:\n",
    "                    if next_state not in self.maze[state]:\n",
    "\n",
    "                        # Add the geometry of the edges and walls (i.e. the boundaries between\n",
    "                        # adjacent squares that are not connected).\n",
    "                        row_diff, col_diff = np.subtract(next_state, state)\n",
    "                        left = (col + (col_diff > 0)) * scale - 2 * (col_diff != 0)\n",
    "                        right = ((col + 1) - (col_diff < 0)) * scale + 2 * (col_diff != 0)\n",
    "                        top = (5 - (row + (row_diff > 0))) * scale - 2 * (row_diff != 0)\n",
    "                        bottom = (5 - ((row + 1) - (row_diff < 0))) * scale + 2 * (row_diff != 0)\n",
    "\n",
    "                        gfxdraw.filled_polygon(surf, [(left, bottom), (left, top), (right, top), (right, bottom)], (255, 255, 255))\n",
    "\n",
    "        # Add the geometry of the goal square to the viewer.\n",
    "        left, right, top, bottom = scale * 4 + 10, scale * 5 - 10, scale - 10, 10\n",
    "        gfxdraw.filled_polygon(surf, [(left, bottom), (left, top), (right, top), (right, bottom)], (40, 199, 172))\n",
    "\n",
    "        # Add the geometry of the agent to the viewer.\n",
    "        agent_row = int(screen_size - scale * (self.state[0] + .5))\n",
    "        agent_col = int(scale * (self.state[1] + .5))\n",
    "        gfxdraw.filled_circle(surf, agent_col, agent_row, int(scale * .6 / 2), (228, 63, 90))\n",
    "\n",
    "        surf = pygame.transform.flip(surf, False, True)\n",
    "        self.screen.blit(surf, (0, 0))\n",
    "\n",
    "        return np.transpose(\n",
    "                np.array(pygame.surfarray.pixels3d(self.screen)), axes=(1, 0, 2)\n",
    "            )\n",
    "\n",
    "    def close(self) -> None:\n",
    "        if self.screen is not None:\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()\n",
    "            self.screen = None\n",
    "\n",
    "    def compute_reward(self, state: Tuple[int, int], action: int) -> float:\n",
    "        next_state = self._get_next_state(state, action)\n",
    "        if self.shaped_rewards:\n",
    "            return - (self.distances[next_state] / self.distances.max())\n",
    "        return - float(state != self.goal)\n",
    "\n",
    "    def simulate_step(self, state: Tuple[int, int], action: int):\n",
    "        reward = self.compute_reward(state, action)\n",
    "        next_state = self._get_next_state(state, action)\n",
    "        done = next_state == self.goal\n",
    "        info = {}\n",
    "        return next_state, reward, done, info\n",
    "\n",
    "    def _get_next_state(self, state: Tuple[int, int], action: int) -> Tuple[int, int]:\n",
    "        if action == 0:\n",
    "            next_state = (state[0] - 1, state[1])\n",
    "        elif action == 1:\n",
    "            next_state = (state[0], state[1] + 1)\n",
    "        elif action == 2:\n",
    "            next_state = (state[0] + 1, state[1])\n",
    "        elif action == 3:\n",
    "            next_state = (state[0], state[1] - 1)\n",
    "        else:\n",
    "            raise ValueError(\"Action value not supported:\", action)\n",
    "        if next_state in self.maze[state]:\n",
    "            return next_state\n",
    "        return state\n",
    "\n",
    "    @staticmethod\n",
    "    def _create_maze(size: int) -> Dict[Tuple[int, int], Iterable[Tuple[int, int]]]:\n",
    "        maze = {(row, col): [(row - 1, col), (row + 1, col), (row, col - 1), (row, col + 1)]\n",
    "                for row in range(size) for col in range(size)}\n",
    "\n",
    "        left_edges = [[(row, 0), (row, -1)] for row in range(size)]\n",
    "        right_edges = [[(row, size - 1), (row, size)] for row in range(size)]\n",
    "        upper_edges = [[(0, col), (-1, col)] for col in range(size)]\n",
    "        lower_edges = [[(size - 1, col), (size, col)] for col in range(size)]\n",
    "        walls = [\n",
    "            [(1, 0), (1, 1)], [(2, 0), (2, 1)], [(3, 0), (3, 1)],\n",
    "            [(1, 1), (1, 2)], [(2, 1), (2, 2)], [(3, 1), (3, 2)],\n",
    "            [(3, 1), (4, 1)], [(0, 2), (1, 2)], [(1, 2), (1, 3)],\n",
    "            [(2, 2), (3, 2)], [(2, 3), (3, 3)], [(2, 4), (3, 4)],\n",
    "            [(4, 2), (4, 3)], [(1, 3), (1, 4)], [(2, 3), (2, 4)],\n",
    "        ]\n",
    "\n",
    "        obstacles = upper_edges + lower_edges + left_edges + right_edges + walls\n",
    "\n",
    "        for src, dst in obstacles:\n",
    "            maze[src].remove(dst)\n",
    "\n",
    "            if dst in maze:\n",
    "                maze[dst].remove(src)\n",
    "\n",
    "        return maze\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_distances(goal: Tuple[int, int],\n",
    "                           maze: Dict[Tuple[int, int], Iterable[Tuple[int, int]]]) -> np.ndarray:\n",
    "        distances = np.full((5, 5), np.inf)\n",
    "        visited = set()\n",
    "        distances[goal] = 0.\n",
    "\n",
    "        while visited != set(maze):\n",
    "            sorted_dst = [(v // 5, v % 5) for v in distances.argsort(axis=None)]\n",
    "            closest = next(x for x in sorted_dst if x not in visited)\n",
    "            visited.add(closest)\n",
    "\n",
    "            for neighbour in maze[closest]:\n",
    "                distances[neighbour] = min(distances[neighbour], distances[closest] + 1)\n",
    "        return distances\n",
    "\n",
    "\n",
    "def display_video(frames):\n",
    "    # Copied from: https://colab.research.google.com/github/deepmind/dm_control/blob/master/tutorial.ipynb\n",
    "    orig_backend = matplotlib.get_backend()\n",
    "    matplotlib.use('Agg')\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "    matplotlib.use(orig_backend)\n",
    "    ax.set_axis_off()\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_position([0, 0, 1, 1])\n",
    "    im = ax.imshow(frames[0])\n",
    "    def update(frame):\n",
    "        im.set_data(frame)\n",
    "        return [im]\n",
    "    anim = animation.FuncAnimation(fig=fig, func=update, frames=frames,\n",
    "                                    interval=50, blit=True, repeat=False)\n",
    "    return HTML(anim.to_html5_video())\n",
    "\n",
    "\n",
    "def seed_everything(env: gym.Env, seed: int = 42) -> None:\n",
    "    #|env.seed(seed)\n",
    "    env.action_space.seed(seed)\n",
    "    env.observation_space.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "\n",
    "\n",
    "def plot_stats(stats):\n",
    "    rows = len(stats)\n",
    "    cols = 1\n",
    "\n",
    "    fig, ax = plt.subplots(rows, cols, figsize=(12, 6))\n",
    "\n",
    "    for i, key in enumerate(stats):\n",
    "        vals = stats[key]\n",
    "        vals = [np.mean(vals[i-10:i+10]) for i in range(10, len(vals)-10)]\n",
    "        if len(stats) > 1:\n",
    "            ax[i].plot(range(len(vals)), vals)\n",
    "            ax[i].set_title(key, size=18)\n",
    "        else:\n",
    "            ax.plot(range(len(vals)), vals)\n",
    "            ax.set_title(key, size=18)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def test_policy_network(env, policy, episodes=10):\n",
    "    frames = []\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        frames.append(env.render())\n",
    "\n",
    "        while not done:\n",
    "            state = torch.from_numpy(state).unsqueeze(0).float()\n",
    "            action = policy(state).multinomial(1).item()\n",
    "            next_state, _, done, _ = env.step(action)\n",
    "            img = env.render()\n",
    "            frames.append(img)\n",
    "            state = next_state\n",
    "\n",
    "    return display_video(frames)\n",
    "\n",
    "\n",
    "def plot_action_probs(probs, labels):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.bar(labels, probs, color ='orange')\n",
    "    plt.title(r\"$\\pi(s)$\", size=16)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RlB0Tbp07vV6"
   },
   "source": [
    "## Import the necessary software libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "2OnbUU8t7vV7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import gymnasium\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch import nn as nn\n",
    "from torch.optim import AdamW\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pPEwlOrt7vV8"
   },
   "source": [
    "## Create and preprocess the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j37j_pOh7vV8"
   },
   "source": [
    "### Create the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "RDViC8L47vV8"
   },
   "outputs": [],
   "source": [
    "env = gym.make('Acrobot-v1', render_mode='rgb_array')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "LuJ9Hx4E7vV8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State dimensions: 6. Actions: 3\n",
      "Sample state: (array([0.99677855, 0.08020269, 0.995878  , 0.09070294, 0.01738493,\n",
      "       0.04968391], dtype=float32), {})\n"
     ]
    }
   ],
   "source": [
    "dims = env.observation_space.shape[0]\n",
    "actions = env.action_space.n\n",
    "\n",
    "print(f\"State dimensions: {dims}. Actions: {actions}\")\n",
    "print(f\"Sample state: {env.reset()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "QVHSTC827vV8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe57e77a990>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(env.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8piDIP4E7vV9"
   },
   "source": [
    "### Prepare the environment to work with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "bBqxt5hP7vV9"
   },
   "outputs": [],
   "source": [
    "class PreprocessEnv(gym.vector.VectorWrapper):\n",
    "\n",
    "    def __init__(self, env):\n",
    "        #gym.Wrapper.__init__(self, env)\n",
    "        super().__init__(env)\n",
    "\n",
    "    def reset(self):\n",
    "        state, _ = self.env.reset()\n",
    "        return torch.from_numpy(state).float()\n",
    "\n",
    "    def step(self, actions):\n",
    "        actions = actions.squeeze().numpy()\n",
    "        next_state, reward, done, info, _ = self.env.step(actions)\n",
    "        next_state = torch.from_numpy(next_state).float()\n",
    "        reward = torch.tensor(reward).unsqueeze(1).float()\n",
    "        done = torch.tensor(done).unsqueeze(1)\n",
    "        return next_state, reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "jkRkODiA7vV-"
   },
   "outputs": [],
   "source": [
    "num_envs = 12\n",
    "parallel_env = gym.make_vec('Acrobot-v1', num_envs=num_envs, vectorization_mode=\"async\")\n",
    "seed_everything(parallel_env)\n",
    "parallel_env = PreprocessEnv(parallel_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H_35Rcxy7vV-"
   },
   "source": [
    "### Create the policy $\\pi(s)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "rF1_C5Xb7vV-"
   },
   "outputs": [],
   "source": [
    "# actor:\n",
    "policy = nn.Sequential(\n",
    "    nn.Linear(dims, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, actions), # range [-inf, inf] -> [0, 1]\n",
    "    nn.Softmax(dim=-1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LPY8NEB17vV-"
   },
   "source": [
    "### Create the value network $v(s)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "EKD6vk4i7vV-"
   },
   "outputs": [],
   "source": [
    "# critic:\n",
    "value_net = nn.Sequential(\n",
    "    nn.Linear(dims, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 1) # range [-inf, inf]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SH5RVyq-7vV-"
   },
   "source": [
    "## Implement the algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lI3Zju7u7vV-"
   },
   "outputs": [],
   "source": [
    "def actor_critic(policy, value_net, episodes, alpha=1e-4, gamma=0.99):\n",
    "    policy_optim = AdamW(policy.parameters(), lr=alpha)\n",
    "    value_optim = AdamW(value_net.parameters(), lr=alpha)\n",
    "    stats = {'Actor Loss': [], 'Critic Loss': [], 'Returns': []}\n",
    "\n",
    "    for episode in tqdm(range(1, episodes + 1)):\n",
    "        state = parallel_env.reset()\n",
    "        done_b = torch.zeros((num_envs, 1), dtype=torch.bool)\n",
    "        ep_return = torch.zeros((num_envs, 1))\n",
    "        I = 1.\n",
    "\n",
    "        while not done_b.all():\n",
    "            action = policy(state).multinomial(1).detach()\n",
    "            next_state, reward, done, _ = parallel_env.step(action)\n",
    "\n",
    "            # updating value network:\n",
    "            value = value_net(state)\n",
    "            target = reward + ~done * gamma * value_net(next_state).detach()\n",
    "            critic_loss = F.mse_loss(value, target)\n",
    "            value_net.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            value_optim.step()\n",
    "\n",
    "            # updating policy network:\n",
    "            advantage = (target - value).detach()\n",
    "            probs = policy(state)\n",
    "            log_probs = torch.log(probs + 1e-6)\n",
    "            action_log_prob = log_probs.gather(1, action)\n",
    "            entropy = - torch.sum(probs * log_probs, dim=-1, keepdim=True)\n",
    "            actor_loss = -I * action_log_prob * advantage - 0.01 * entropy\n",
    "            actor_loss = actor_loss.mean()\n",
    "            policy.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            policy_optim.step()\n",
    "\n",
    "            ep_return += reward\n",
    "            done_b |= done\n",
    "            state = next_state\n",
    "            I = I * gamma\n",
    "\n",
    "        stats['Actor Loss'].append(actor_loss.item())\n",
    "        stats['Critic Loss'].append(critic_loss.item())\n",
    "        stats['Returns'].append(ep_return.mean().item())\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "eEWU63Z07vV-",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▋                                                                                                                                                   | 1/200 [06:52<22:48:44, 412.69s/it]\n",
      "Exception ignored in: <function VectorEnv.__del__ at 0x7fe57f8a1260>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gpaik/.conda/envs/pytorch/lib/python3.12/site-packages/gymnasium/vector/vector_env.py\", line 324, in __del__\n",
      "    self.close()\n",
      "  File \"/home/gpaik/.conda/envs/pytorch/lib/python3.12/site-packages/gymnasium/vector/vector_env.py\", line 387, in close\n",
      "    return self.env.close(**kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/gpaik/.conda/envs/pytorch/lib/python3.12/site-packages/gymnasium/vector/vector_env.py\", line 222, in close\n",
      "    self.close_extras(**kwargs)\n",
      "  File \"/home/gpaik/.conda/envs/pytorch/lib/python3.12/site-packages/gymnasium/vector/async_vector_env.py\", line 588, in close_extras\n",
      "    pipe.send((\"close\", None))\n",
      "  File \"/home/gpaik/.conda/envs/pytorch/lib/python3.12/multiprocessing/connection.py\", line 206, in send\n",
      "    self._send_bytes(_ForkingPickler.dumps(obj))\n",
      "  File \"/home/gpaik/.conda/envs/pytorch/lib/python3.12/multiprocessing/connection.py\", line 427, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/gpaik/.conda/envs/pytorch/lib/python3.12/multiprocessing/connection.py\", line 384, in _send\n",
      "    n = write(self._handle, buf)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "/home/gpaik/.conda/envs/pytorch/lib/python3.12/site-packages/gymnasium/vector/async_vector_env.py:573: UserWarning: \u001b[33mWARN: Calling `close` while waiting for a pending call to `step` to complete.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/gpaik/.conda/envs/pytorch/lib/python3.12/site-packages/gymnasium/vector/async_vector_env.py:422: UserWarning: \u001b[31mERROR: Received the following error from Worker-3 - Shutting it down\u001b[0m\n",
      "  self._raise_if_errors(successes)\n",
      "/home/gpaik/.conda/envs/pytorch/lib/python3.12/site-packages/gymnasium/vector/async_vector_env.py:422: UserWarning: \u001b[31mERROR: Traceback (most recent call last):\n",
      "  File \"/home/gpaik/.conda/envs/pytorch/lib/python3.12/site-packages/gymnasium/vector/async_vector_env.py\", line 701, in _async_worker\n",
      "    command, data = pipe.recv()\n",
      "                    ^^^^^^^^^^^\n",
      "  File \"/home/gpaik/.conda/envs/pytorch/lib/python3.12/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/gpaik/.conda/envs/pytorch/lib/python3.12/multiprocessing/connection.py\", line 430, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "          ^^^^^^^^^^^^^\n",
      "  File \"/home/gpaik/.conda/envs/pytorch/lib/python3.12/multiprocessing/connection.py\", line 395, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\u001b[0m\n",
      "  self._raise_if_errors(successes)\n",
      "/home/gpaik/.conda/envs/pytorch/lib/python3.12/site-packages/gymnasium/vector/async_vector_env.py:422: UserWarning: \u001b[31mERROR: Received the following error from Worker-8 - Shutting it down\u001b[0m\n",
      "  self._raise_if_errors(successes)\n",
      "/home/gpaik/.conda/envs/pytorch/lib/python3.12/site-packages/gymnasium/vector/async_vector_env.py:422: UserWarning: \u001b[31mERROR: Received the following error from Worker-4 - Shutting it down\u001b[0m\n",
      "  self._raise_if_errors(successes)\n",
      "/home/gpaik/.conda/envs/pytorch/lib/python3.12/site-packages/gymnasium/vector/async_vector_env.py:422: UserWarning: \u001b[31mERROR: Received the following error from Worker-6 - Shutting it down\u001b[0m\n",
      "  self._raise_if_errors(successes)\n",
      "/home/gpaik/.conda/envs/pytorch/lib/python3.12/site-packages/gymnasium/vector/async_vector_env.py:422: UserWarning: \u001b[31mERROR: Received the following error from Worker-7 - Shutting it down\u001b[0m\n",
      "  self._raise_if_errors(successes)\n",
      "/home/gpaik/.conda/envs/pytorch/lib/python3.12/site-packages/gymnasium/vector/async_vector_env.py:422: UserWarning: \u001b[31mERROR: Received the following error from Worker-2 - Shutting it down\u001b[0m\n",
      "  self._raise_if_errors(successes)\n",
      "/home/gpaik/.conda/envs/pytorch/lib/python3.12/site-packages/gymnasium/vector/async_vector_env.py:422: UserWarning: \u001b[31mERROR: Received the following error from Worker-1 - Shutting it down\u001b[0m\n",
      "  self._raise_if_errors(successes)\n",
      "/home/gpaik/.conda/envs/pytorch/lib/python3.12/site-packages/gymnasium/vector/async_vector_env.py:422: UserWarning: \u001b[31mERROR: Received the following error from Worker-0 - Shutting it down\u001b[0m\n",
      "  self._raise_if_errors(successes)\n",
      "/home/gpaik/.conda/envs/pytorch/lib/python3.12/site-packages/gymnasium/vector/async_vector_env.py:422: UserWarning: \u001b[31mERROR: Received the following error from Worker-9 - Shutting it down\u001b[0m\n",
      "  self._raise_if_errors(successes)\n",
      "/home/gpaik/.conda/envs/pytorch/lib/python3.12/site-packages/gymnasium/vector/async_vector_env.py:422: UserWarning: \u001b[31mERROR: Traceback (most recent call last):\n",
      "  File \"/home/gpaik/.conda/envs/pytorch/lib/python3.12/site-packages/gymnasium/vector/async_vector_env.py\", line 723, in _async_worker\n",
      "    ) = env.step(data)\n",
      "        ^^^^^^^^^^^^^^\n",
      "  File \"/home/gpaik/.conda/envs/pytorch/lib/python3.12/site-packages/gymnasium/wrappers/common.py\", line 125, in step\n",
      "    observation, reward, terminated, truncated, info = self.env.step(action)\n",
      "                                                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/gpaik/.conda/envs/pytorch/lib/python3.12/site-packages/gymnasium/wrappers/common.py\", line 393, in step\n",
      "    return super().step(action)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/gpaik/.conda/envs/pytorch/lib/python3.12/site-packages/gymnasium/core.py\", line 322, in step\n",
      "    return self.env.step(action)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/gpaik/.conda/envs/pytorch/lib/python3.12/site-packages/gymnasium/wrappers/common.py\", line 285, in step\n",
      "    return self.env.step(action)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/gpaik/.conda/envs/pytorch/lib/python3.12/site-packages/gymnasium/envs/classic_control/acrobot.py\", line 225, in step\n",
      "    terminated = self._terminal()\n",
      "                 ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/gpaik/.conda/envs/pytorch/lib/python3.12/site-packages/gymnasium/envs/classic_control/acrobot.py\", line 243, in _terminal\n",
      "    return bool(-cos(s[0]) - cos(s[1] + s[0]) > 1.0)\n",
      "                             ^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\u001b[0m\n",
      "  self._raise_if_errors(successes)\n",
      "/home/gpaik/.conda/envs/pytorch/lib/python3.12/site-packages/gymnasium/vector/async_vector_env.py:422: UserWarning: \u001b[31mERROR: Raising the last exception back to the main process.\u001b[0m\n",
      "  self._raise_if_errors(successes)\n",
      "Exception ignored in: <function VectorEnv.__del__ at 0x7fe57f8a1260>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gpaik/.conda/envs/pytorch/lib/python3.12/site-packages/gymnasium/vector/vector_env.py\", line 324, in __del__\n",
      "    self.close()\n",
      "  File \"/home/gpaik/.conda/envs/pytorch/lib/python3.12/site-packages/gymnasium/vector/vector_env.py\", line 387, in close\n",
      "    return self.env.close(**kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/gpaik/.conda/envs/pytorch/lib/python3.12/site-packages/gymnasium/vector/vector_env.py\", line 222, in close\n",
      "    self.close_extras(**kwargs)\n",
      "  File \"/home/gpaik/.conda/envs/pytorch/lib/python3.12/site-packages/gymnasium/vector/async_vector_env.py\", line 577, in close_extras\n",
      "    function(timeout)\n",
      "  File \"/home/gpaik/.conda/envs/pytorch/lib/python3.12/site-packages/gymnasium/vector/async_vector_env.py\", line 422, in step_wait\n",
      "    self._raise_if_errors(successes)\n",
      "  File \"/home/gpaik/.conda/envs/pytorch/lib/python3.12/site-packages/gymnasium/vector/async_vector_env.py\", line 676, in _raise_if_errors\n",
      "    raise exctype(value)\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m stats \u001b[38;5;241m=\u001b[39m \u001b[43mactor_critic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[52], line 19\u001b[0m, in \u001b[0;36mactor_critic\u001b[0;34m(policy, value_net, episodes, alpha, gamma)\u001b[0m\n\u001b[1;32m     17\u001b[0m value \u001b[38;5;241m=\u001b[39m value_net(state)\n\u001b[1;32m     18\u001b[0m target \u001b[38;5;241m=\u001b[39m reward \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m~\u001b[39mdone \u001b[38;5;241m*\u001b[39m gamma \u001b[38;5;241m*\u001b[39m value_net(next_state)\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m---> 19\u001b[0m critic_loss \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m value_net\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     21\u001b[0m critic_loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.12/site-packages/torch/nn/functional.py:3791\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3789\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3791\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3792\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mmse_loss(\n\u001b[1;32m   3793\u001b[0m     expanded_input, expanded_target, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[1;32m   3794\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.12/site-packages/torch/functional.py:76\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[38;5;241m*\u001b[39mtensors)\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "stats = actor_critic(policy, value_net, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DoxzCbPz7vV-"
   },
   "source": [
    "## Show results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3cmyUHP67vV-"
   },
   "source": [
    "### Show execution stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oV46xCdU7vV-"
   },
   "outputs": [],
   "source": [
    "plot_stats(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KD1Khhk17vV-"
   },
   "source": [
    "### Test the resulting agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w6EwEhPd7vV_"
   },
   "outputs": [],
   "source": [
    "test_policy_network(env, actor, episodes=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UHS56xgc7vV_"
   },
   "source": [
    "## Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yk1oi1-_7vV_"
   },
   "source": [
    "[[1] Reinforcement Learning: An Introduction. Ch.13](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
